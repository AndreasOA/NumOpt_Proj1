{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Group 7<br>\n",
    "Participants information in alphabetical order</i>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">Florian</td>\n",
    "    <td style = \"text-align: left\">Rothkegel</td>\n",
    "    <td style = \"text-align: left\">k11908775</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Andreas</td>\n",
    "    <td style = \"text-align: left\">Oberdammer</td>\n",
    "    <td style = \"text-align: left\">k11908776</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">3</td>\n",
    "    <td style = \"text-align: left\">Martin</td>\n",
    "    <td style = \"text-align: left\">Zwiffl</td>\n",
    "    <td style = \"text-align: left\">k11910668</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">4</td>\n",
    "    <td style = \"text-align: left\">Martin</td>\n",
    "    <td style = \"text-align: left\">Stockinger</td>\n",
    "    <td style = \"text-align: left\">k01035089</td>\n",
    "    </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">5</td>\n",
    "    <td style = \"text-align: left\">Alexander</td>\n",
    "    <td style = \"text-align: left\">Mair</td>\n",
    "    <td style = \"text-align: left\">k11916624</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">6</td>\n",
    "    <td style = \"text-align: left\">Dominik</td>\n",
    "    <td style = \"text-align: left\">Zauner</td>\n",
    "    <td style = \"text-align: left\">***</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
    "<i>Describe how to install additional packages, if you have some, here</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for stopping criterium\n",
    "def stopping_criteria(alpha, iteration, epsilon=1e-5, max_steps=1e6):\n",
    "    # @alpha: alpha in this case can be the derivative/gradient or alpha itself\n",
    "    # @iteration: current iteration of the algorithm\n",
    "    # @epsilon: stopping value for alpha/graident. if alpha is smaller or equal\n",
    "    #           to epsilon the algorithm has to stop\n",
    "    # @max_steps: stopping value for iteration. if iteration is higher or equal\n",
    "    #             to max_steps the algorithm has to stop\n",
    "    # ToDo: Adjustments? Different Name for alpha? Should we parse more?\n",
    "    #       x_old/x_new, y_new/y_old?\n",
    "    return ((alpha <= epsilon) or (iteration >= max_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for stabilising goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
    "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for invertion goes here\n",
    "def lu_deco_inverse(A):\n",
    "    n = A.shape[0]\n",
    "    a = np.copy(A.astype(float))\n",
    "    p = np.eye(n)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        j = i + np.argmax(np.abs(a[i:, i]))     # maximum in column\n",
    "        if np.abs(a[i, j]) < 1e-8:\n",
    "            raise ValueError(\"Matrix is singular\")\n",
    "\n",
    "        if i != j:\n",
    "            a[[i, j]] = a[[j, i]]\n",
    "            p[[i, j]] = p[[j, i]]\n",
    "\n",
    "        for i_row in range(i + 1, n):\n",
    "            a[i_row, i] = a[i_row:, i] / a[i, i]\n",
    "        for i_row in range(i + 1, n):\n",
    "            for i_col in range(i + 1, n):\n",
    "                a[i_row, i_col] = a[i_row, i_col] - np.outer(a[i_row, i], a[i, i_col])\n",
    "\n",
    "    a_inv = np.copy(np.dot(p, np.eye(n)))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            a_inv[i] -= np.dot(a[i, j], a_inv[j])         # Forward substitution\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        for j in range(i + 1, n):\n",
    "            a_inv[i] = (a_inv[i] - np.dot(a[i, j], a_inv[j])) / a[i, i]       # Backward substitution\n",
    "\n",
    "    return a_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for gradient approximation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def steepest_descent_multi(xk, minimizer, gradient, lr=1e-3, max_steps=1e6, epsilon=1e-5,\n",
    "         activate_new_stop_crit=False,\n",
    "         activate_pre_scaling=False,\n",
    "         activate_alt_gradient=False,\n",
    "         activate_new_matrix_inversion=False):\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        grad = gradient(xk)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "        if activate_new_stop_crit and stopping_criteria(grad_norm, i, epsilon, max_steps):\n",
    "            break\n",
    "\n",
    "        elif not activate_new_stop_crit and grad_norm <= epsilon:\n",
    "            break\n",
    "\n",
    "        xk=xk-(lr * grad)\n",
    "        i += 1\n",
    "\n",
    "    print(\"performed \" + str(i+1) + \" iterations. Final norm of gradient: \" + str(grad_norm))\n",
    "\n",
    "    return xk\n",
    "\n",
    "def steepest_descent_uni(xk, minimizers, derivative, lr=1e-7, max_steps=1e6, epsilon=1e-5,\n",
    "         activate_new_stop_crit=False,\n",
    "         activate_pre_scaling=False,\n",
    "         activate_alt_gradient=False,\n",
    "         activate_new_matrix_inversion=False):\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        der = derivative(xk)\n",
    "\n",
    "        if activate_new_stop_crit and stopping_criteria(abs(der), i, epsilon, max_steps):\n",
    "            break\n",
    "\n",
    "        elif not activate_new_stop_crit and abs(der) <= epsilon:\n",
    "            break\n",
    "\n",
    "        xk=xk-(lr * der)\n",
    "        i += 1\n",
    "    print(\"performed \" + str(i+1) + \" iterations. Final derivative: \" + str(der))\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def get_random_A_b_minimizer(seed):\n",
    "    np.random.seed(seed)\n",
    "    A=np.random.randint(0, 2, (11, 11))\n",
    "    A=A@A.T\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    minimizer=np.random.randint(0, 10, (11, 1))\n",
    "\n",
    "    b=A@minimizer\n",
    "\n",
    "    return A, b, minimizer\n",
    "\n",
    "A_1, b_1, minimizer_1 = get_random_A_b_minimizer(0)\n",
    "A_2, b_2, minimizer_2 = get_random_A_b_minimizer(2)\n",
    "A_3, b_3, minimizer_3 = get_random_A_b_minimizer(3)\n",
    "A_4, b_4, minimizer_4 = get_random_A_b_minimizer(4)\n",
    "A_5, b_5, minimizer_5 = get_random_A_b_minimizer(6)\n",
    "A_6, b_6, minimizer_6 = get_random_A_b_minimizer(12)\n",
    "A_7, b_7, minimizer_7 = get_random_A_b_minimizer(15)\n",
    "\n",
    "def problem_1_q(x):\n",
    "    return (x.T@A_1@x)/2-b_1.T@x\n",
    "\n",
    "def grad_problem_1_q(x):\n",
    "    return np.dot(A_1,x)-b_1\n",
    "\n",
    "def hessian_problem_1_q(x):\n",
    "    return A_1\n",
    "\n",
    "def problem_2_q(x):\n",
    "    return (x.T@A_2@x)/2-b_2.T@x\n",
    "\n",
    "def grad_problem_2_q(x):\n",
    "    return A_2@x-b_2\n",
    "\n",
    "def hessian_problem_2_q(x):\n",
    "    return A_2\n",
    "\n",
    "def problem_3_q(x):\n",
    "    return (x.T@A_3@x)/2-b_3.T@x\n",
    "\n",
    "def grad_problem_3_q(x):\n",
    "    return A_3@x-b_3\n",
    "\n",
    "def hessian_problem_3_q(x):\n",
    "    return A_3\n",
    "\n",
    "def problem_4_q(x):\n",
    "    return (x.T@A_4@x)/2-b_4.T@x\n",
    "\n",
    "def grad_problem_4_q(x):\n",
    "    return A_4@x-b_4\n",
    "\n",
    "def hessian_problem_4_q(x):\n",
    "    return A_4\n",
    "\n",
    "def problem_5_q(x):\n",
    "    return (x.T@A_5@x)/2-b_5.T@x\n",
    "\n",
    "def grad_problem_5_q(x):\n",
    "    return A_5@x-b_5\n",
    "\n",
    "def hessian_problem_5_q(x):\n",
    "    return A_5\n",
    "\n",
    "def problem_6_q(x):\n",
    "    return (x.T@A_6@x)/2-b_6.T@x\n",
    "\n",
    "def grad_problem_6_q(x):\n",
    "    return A_6@x-b_6\n",
    "\n",
    "def hessian_problem_6_q(x):\n",
    "    return A_6\n",
    "\n",
    "def problem_7_q(x):\n",
    "    return (x.T@A_7@x)/2-b_7.T@x\n",
    "\n",
    "def grad_problem_7_q(x):\n",
    "    return A_7@x-b_7\n",
    "\n",
    "def hessian_problem_7_q(x):\n",
    "    return A_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><b>Note:</b> After every test print out the resulsts. \n",
    "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
    "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_0: numpy 1D array, corresponds to initial point\n",
    "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
    "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
    "                                      **args\n",
    "       Function f returns a scalar output.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
    "                                         function f,\n",
    "                                         args (which are submitted, because you might need\n",
    "                                              to call f(x,**args) inside your gradient function implementation). \n",
    "          Function grad approximates gradient at given point and returns a 1d np array.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    args: dictionary, additional (except of x) arguments to function f\n",
    "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Initial x is :\\t\\t{x_0}')\n",
    "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
    "    print(f'Approximated x is :\\t{x_appr}')\n",
    "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
    "    f_opt = f(x_optimal)\n",
    "    f_appr = f(x_appr)\n",
    "    print(f'Function value in optimal point:\\t{f_opt}')\n",
    "    print(f'Function value in approximated point:   {f_appr}')\n",
    "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
    "    print(f'Gradient approximation in optimal point is:\\n{grad(x_optimal)}\\n')\n",
    "    grad_appr = grad(x_appr)\n",
    "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
    "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performed 218415 iterations. Final norm of gradient: 9.999606897549827e-07\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]\n",
      " [7]]\n",
      "Approximated x is :\t[[ 5.00001026e+00]\n",
      " [-1.79568110e-05]\n",
      " [ 2.99999751e+00]\n",
      " [ 3.00000247e+00]\n",
      " [ 7.00000836e+00]\n",
      " [ 8.99997703e+00]\n",
      " [ 3.00000616e+00]\n",
      " [ 5.00000229e+00]\n",
      " [ 1.99999532e+00]\n",
      " [ 4.00000136e+00]\n",
      " [ 6.99999616e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4286.5]]\n",
      "Function value in approximated point:   [[-4286.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 3.06544734e-07]\n",
      " [-5.36698593e-07]\n",
      " [-7.44871045e-08]\n",
      " [ 7.38847916e-08]\n",
      " [ 2.49925790e-07]\n",
      " [-6.86647695e-07]\n",
      " [ 1.84058166e-07]\n",
      " [ 6.83865835e-08]\n",
      " [-1.39817530e-07]\n",
      " [ 4.05682101e-08]\n",
      " [-1.14900928e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 151746 iterations. Final norm of gradient: 9.99980222159716e-07\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [8]\n",
      " [6]\n",
      " [2]\n",
      " [8]\n",
      " [7]\n",
      " [2]\n",
      " [1]\n",
      " [5]\n",
      " [4]\n",
      " [4]]\n",
      "Approximated x is :\t[[8.00000304]\n",
      " [7.99999447]\n",
      " [6.00000725]\n",
      " [2.00001065]\n",
      " [8.00000003]\n",
      " [6.99999164]\n",
      " [2.00000034]\n",
      " [0.99999213]\n",
      " [4.99999349]\n",
      " [4.00000009]\n",
      " [4.00000339]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4260.]]\n",
      "Function value in approximated point:   [[-4260.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.53848248e-07]\n",
      " [-2.79358545e-07]\n",
      " [ 3.66501041e-07]\n",
      " [ 5.38206862e-07]\n",
      " [ 1.36014933e-09]\n",
      " [-4.22210320e-07]\n",
      " [ 1.72660748e-08]\n",
      " [-3.97802097e-07]\n",
      " [-3.28768884e-07]\n",
      " [ 4.67804284e-09]\n",
      " [ 1.71293408e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 273454 iterations. Final norm of gradient: 9.999945792912107e-07\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [9]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [5]\n",
      " [3]\n",
      " [9]\n",
      " [9]\n",
      " [5]]\n",
      "Approximated x is :\t[[7.99999098e+00]\n",
      " [8.99998335e+00]\n",
      " [2.99999902e+00]\n",
      " [7.99999032e+00]\n",
      " [8.00000180e+00]\n",
      " [9.60588278e-06]\n",
      " [5.00001029e+00]\n",
      " [3.00000707e+00]\n",
      " [8.99999755e+00]\n",
      " [9.00001115e+00]\n",
      " [4.99998034e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-5551.5]]\n",
      "Function value in approximated point:   [[-5551.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-2.58135600e-07]\n",
      " [-4.76428283e-07]\n",
      " [-2.80368795e-08]\n",
      " [-2.77025578e-07]\n",
      " [ 5.15767624e-08]\n",
      " [ 2.74841227e-07]\n",
      " [ 2.94551512e-07]\n",
      " [ 2.02377521e-07]\n",
      " [-7.02063971e-08]\n",
      " [ 3.18887828e-07]\n",
      " [-5.62413124e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 141585 iterations. Final norm of gradient: 9.999993048571423e-07\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[7]\n",
      " [5]\n",
      " [1]\n",
      " [8]\n",
      " [7]\n",
      " [8]\n",
      " [2]\n",
      " [9]\n",
      " [7]\n",
      " [7]\n",
      " [7]]\n",
      "Approximated x is :\t[[7.00000879]\n",
      " [4.99999766]\n",
      " [1.00000125]\n",
      " [8.00000468]\n",
      " [6.99999094]\n",
      " [8.00000356]\n",
      " [2.00000495]\n",
      " [8.99999659]\n",
      " [6.99999324]\n",
      " [6.99999727]\n",
      " [7.00000526]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-6486.5]]\n",
      "Function value in approximated point:   [[-6486.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 4.92911397e-07]\n",
      " [-1.31340045e-07]\n",
      " [ 7.02435159e-08]\n",
      " [ 2.62232419e-07]\n",
      " [-5.08166750e-07]\n",
      " [ 1.99443662e-07]\n",
      " [ 2.77522503e-07]\n",
      " [-1.91515198e-07]\n",
      " [-3.79270290e-07]\n",
      " [-1.53116616e-07]\n",
      " [ 2.95106901e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 1570162 iterations. Final norm of gradient: 9.999959535860875e-07\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[9]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [9]\n",
      " [1]\n",
      " [9]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [8]]\n",
      "Approximated x is :\t[[9.00012854e+00]\n",
      " [2.99987603e+00]\n",
      " [3.99983866e+00]\n",
      " [1.29214499e-04]\n",
      " [9.00010462e+00]\n",
      " [9.99891362e-01]\n",
      " [9.00003946e+00]\n",
      " [1.00007061e+00]\n",
      " [3.99988470e+00]\n",
      " [9.99972600e-01]\n",
      " [8.00007389e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4947.]]\n",
      "Function value in approximated point:   [[-4947.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 3.65888496e-07]\n",
      " [-3.52886332e-07]\n",
      " [-4.59273821e-07]\n",
      " [ 3.67816625e-07]\n",
      " [ 2.97806906e-07]\n",
      " [-3.09244399e-07]\n",
      " [ 1.12311540e-07]\n",
      " [ 2.00985198e-07]\n",
      " [-3.28202134e-07]\n",
      " [-7.79953950e-08]\n",
      " [ 2.10326732e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 1001885 iterations. Final norm of gradient: 9.999977423816248e-07\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [5]\n",
      " [9]]\n",
      "Approximated x is :\t[[5.99997106e+00]\n",
      " [1.00008371e+00]\n",
      " [1.99999900e+00]\n",
      " [2.99995667e+00]\n",
      " [3.00003173e+00]\n",
      " [5.25399597e-05]\n",
      " [5.99998338e+00]\n",
      " [9.99945150e-01]\n",
      " [4.00001726e+00]\n",
      " [4.99991437e+00]\n",
      " [9.00006941e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-2939.5]]\n",
      "Function value in approximated point:   [[-2939.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-1.69282629e-07]\n",
      " [ 4.89670242e-07]\n",
      " [-5.84152815e-09]\n",
      " [-2.53499593e-07]\n",
      " [ 1.85606865e-07]\n",
      " [ 3.07349652e-07]\n",
      " [-9.72361249e-08]\n",
      " [-3.20865183e-07]\n",
      " [ 1.00971192e-07]\n",
      " [-5.00896931e-07]\n",
      " [ 4.06016994e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 248202 iterations. Final norm of gradient: 9.999726248793472e-07\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [5]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [5]\n",
      " [6]\n",
      " [1]\n",
      " [7]\n",
      " [0]]\n",
      "Approximated x is :\t[[ 8.00001024e+00]\n",
      " [ 5.00001528e+00]\n",
      " [ 4.99998716e+00]\n",
      " [ 6.99998593e+00]\n",
      " [-1.33057028e-05]\n",
      " [ 7.00001403e+00]\n",
      " [ 5.00000117e+00]\n",
      " [ 5.99999082e+00]\n",
      " [ 1.00000263e+00]\n",
      " [ 6.99998355e+00]\n",
      " [ 9.74688756e-06]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4039.5]]\n",
      "Function value in approximated point:   [[-4039.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 2.61569880e-07]\n",
      " [ 3.90304990e-07]\n",
      " [-3.27870993e-07]\n",
      " [-3.59420312e-07]\n",
      " [-3.39779007e-07]\n",
      " [ 3.58193716e-07]\n",
      " [ 2.98672660e-08]\n",
      " [-2.34447242e-07]\n",
      " [ 6.70552822e-08]\n",
      " [-4.20057717e-07]\n",
      " [ 2.48899866e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3],[6]])\n",
    "x_hat=steepest_descent_multi(init, minimizer_1, grad_problem_1_q, lr=1.5e-3, epsilon=1e-6)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_2, grad_problem_2_q, lr=1.5e-3, epsilon=1e-6)\n",
    "final_printout(init, minimizer_2, x_hat, problem_2_q, grad_problem_2_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_3, grad_problem_3_q, lr=1.5e-3, epsilon=1e-6)\n",
    "final_printout(init, minimizer_3, x_hat, problem_3_q, grad_problem_3_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_4, grad_problem_4_q, lr=1.5e-3, epsilon=1e-6)\n",
    "final_printout(init, minimizer_4, x_hat, problem_4_q, grad_problem_4_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_5, grad_problem_5_q, lr=1.5e-3, epsilon=1e-6)\n",
    "final_printout(init, minimizer_5, x_hat, problem_5_q, grad_problem_5_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_6, grad_problem_6_q, lr=1.5e-3, epsilon=1e-6)\n",
    "final_printout(init, minimizer_6, x_hat, problem_6_q, grad_problem_6_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_7, grad_problem_7_q, lr=1.5e-3, epsilon=1e-6)\n",
    "final_printout(init, minimizer_7, x_hat, problem_7_q, grad_problem_7_q, {}, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "abc_6 = np.array([1, 20, 300])\n",
    "abc_7 = np.array([4, 50, 600])\n",
    "abc_8 = np.array([7, 80, 900])\n",
    "abc_9 = np.array([1, 100, 200])\n",
    "abc_10 = np.array([2, 120, 200])\n",
    "\n",
    "def get_global_minimizer(problem, stat_points):\n",
    "    y = []\n",
    "    for stat_point in np.array(stat_points).astype(float):\n",
    "        y.append([problem(stat_point), stat_point])\n",
    "    y = np.array(y)\n",
    "    return y[y[:,0].argsort()][0, 1]\n",
    "\n",
    "def problem_6(x):\n",
    "    return -(x*(3*x**3+(-4*abc_6[2]-4*abc_6[1]-4*abc_6[0])*x**2+((6*abc_6[1]+6*abc_6[0])*abc_6[2]+6*abc_6[0]*abc_6[1])*x-12*abc_6[0]*abc_6[1]*abc_6[2]))/12\n",
    "\n",
    "def derivative_problem_6(x):\n",
    "    return (abc_6[0]-x)*(abc_6[1]-x)*(abc_6[2]-x)\n",
    "\n",
    "def second_derivative_problem_6(x):\n",
    "    return -(abc_6[1]-x)*(abc_6[2]-x)-(abc_6[0]-x)*(abc_6[2]-x)-(abc_6[0]-x)*(abc_6[1]-x)\n",
    "\n",
    "def problem_7(x):\n",
    "    return -(x*(3*x**3+(-4*abc_7[2]-4*abc_7[1]-4*abc_7[0])*x**2+((6*abc_7[1]+6*abc_7[0])*abc_7[2]+6*abc_7[0]*abc_7[1])*x-12*abc_7[0]*abc_7[1]*abc_7[2]))/12\n",
    "\n",
    "def derivative_problem_7(x):\n",
    "    return (abc_7[0]-x)*(abc_7[1]-x)*(abc_7[2]-x)\n",
    "\n",
    "def second_derivative_problem_7(x):\n",
    "    return -(abc_7[1]-x)*(abc_7[2]-x)-(abc_7[0]-x)*(abc_7[2]-x)-(abc_7[0]-x)*(abc_7[1]-x)\n",
    "\n",
    "def problem_8(x):\n",
    "    return -(x*(3*x**3+(-4*abc_8[2]-4*abc_8[1]-4*abc_8[0])*x**2+((6*abc_8[1]+6*abc_8[0])*abc_8[2]+6*abc_8[0]*abc_8[1])*x-12*abc_8[0]*abc_8[1]*abc_8[2]))/12\n",
    "\n",
    "def derivative_problem_8(x):\n",
    "    return (abc_8[0]-x)*(abc_8[1]-x)*(abc_8[2]-x)\n",
    "\n",
    "def second_derivative_problem_8(x):\n",
    "    return -(abc_8[1]-x)*(abc_8[2]-x)-(abc_8[0]-x)*(abc_8[2]-x)-(abc_8[0]-x)*(abc_8[1]-x)\n",
    "\n",
    "def problem_9(x):\n",
    "    return -(x*(3*x**3+(-4*abc_9[2]-4*abc_9[1]-4*abc_9[0])*x**2+((6*abc_9[1]+6*abc_9[0])*abc_9[2]+6*abc_9[0]*abc_9[1])*x-12*abc_9[0]*abc_9[1]*abc_9[2]))/12\n",
    "\n",
    "def derivative_problem_9(x):\n",
    "    return (abc_9[0]-x)*(abc_9[1]-x)*(abc_9[2]-x)\n",
    "\n",
    "def second_derivative_problem_9(x):\n",
    "    return -(abc_9[1]-x)*(abc_9[2]-x)-(abc_9[0]-x)*(abc_9[2]-x)-(abc_9[0]-x)*(abc_9[1]-x)\n",
    "\n",
    "def problem_10(x):\n",
    "    return -(x*(3*x**3+(-4*abc_10[2]-4*abc_10[1]-4*abc_10[0])*x**2+((6*abc_10[1]+6*abc_10[0])*abc_10[2]+6*abc_10[0]*abc_10[1])*x-12*abc_10[0]*abc_10[1]*abc_10[2]))/12\n",
    "\n",
    "def derivative_problem_10(x):\n",
    "    return (abc_10[0]-x)*(abc_10[1]-x)*(abc_10[2]-x)\n",
    "\n",
    "def second_derivative_problem_10(x):\n",
    "    return -(abc_10[1]-x)*(abc_10[2]-x)-(abc_10[0]-x)*(abc_10[2]-x)-(abc_10[0]-x)*(abc_10[1]-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performed 420 iterations. Final derivative: 9.629205522687836e-06\n",
      "Initial x is :\t\t100\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t20.00000000181\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   -327999.99999999994\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "9.629205522687836e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n",
      "============================================================================\n",
      "performed 86 iterations. Final derivative: 8.642493298852665e-06\n",
      "Initial x is :\t\t100\n",
      "Optimal x is :\t\t50.0\n",
      "Approximated x is :\t50.0000000003416\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-9062500.0\n",
      "Function value in approximated point:   -9062499.999999998\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "8.642493298852665e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n",
      "============================================================================\n",
      "performed 29 iterations. Final derivative: 5.185634108789749e-06\n",
      "Initial x is :\t\t100\n",
      "Optimal x is :\t\t80.0\n",
      "Approximated x is :\t80.00000000008663\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-53824000.0\n",
      "Function value in approximated point:   -53824000.00000002\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "5.185634108789749e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n",
      "============================================================================\n",
      "performed 222 iterations. Final derivative: -9.837711445469644e-06\n",
      "Initial x is :\t\t90\n",
      "Optimal x is :\t\t100.0\n",
      "Approximated x is :\t99.99999999900629\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-24166666.666666668\n",
      "Function value in approximated point:   -24166666.66666666\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "-9.837711445469644e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n",
      "============================================================================\n",
      "performed 239 iterations. Final derivative: -9.989917089157452e-06\n",
      "Initial x is :\t\t100\n",
      "Optimal x is :\t\t120.0\n",
      "Approximated x is :\t119.99999999894175\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-38016000.0\n",
      "Function value in approximated point:   -38015999.999999985\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "-9.989917089157452e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "x_start=100\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_6), derivative_problem_6, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_start=100\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_7), derivative_problem_7, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_7, abc_7), x_appr=x_hat, f=problem_7, grad=derivative_problem_7, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_start=100\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_8), derivative_problem_8, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_8, abc_8), x_appr=x_hat, f=problem_8, grad=derivative_problem_8, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_start=90\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_9), derivative_problem_9, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_9, abc_9), x_appr=x_hat, f=problem_9, grad=derivative_problem_9, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_start=100\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_10), derivative_problem_10, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_10, abc_10), x_appr=x_hat, f=problem_10, grad=derivative_problem_10, args={}, tolerance=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
    "Template should include sceletons for:<ul>\n",
    "    <li>custom function to optimise over </li> \n",
    "    <li>values initialisation to submit into otimising algorithm </li> \n",
    "    <li>optimiser function call</li> \n",
    "    <li>report print out call</li> </ul><br>\n",
    "Provide descriptions and comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}