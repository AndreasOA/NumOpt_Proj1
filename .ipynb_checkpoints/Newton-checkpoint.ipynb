{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Group 7<br>\n",
    "Participants information in alphabetical order</i>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">Alexander</td>\n",
    "    <td style = \"text-align: left\">Mair</td>\n",
    "    <td style = \"text-align: left\">k11916624</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Andreas</td>\n",
    "    <td style = \"text-align: left\">Oberdammer</td>\n",
    "    <td style = \"text-align: left\">k11908776</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">3</td>\n",
    "    <td style = \"text-align: left\">Dominik</td>\n",
    "    <td style = \"text-align: left\">Zauner</td>\n",
    "    <td style = \"text-align: left\">k11717988</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">4</td>\n",
    "    <td style = \"text-align: left\">Florian</td>\n",
    "    <td style = \"text-align: left\">Rothkegel</td>\n",
    "    <td style = \"text-align: left\">k11908775</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">5</td>\n",
    "    <td style = \"text-align: left\">Martin</td>\n",
    "    <td style = \"text-align: left\">Stockinger</td>\n",
    "    <td style = \"text-align: left\">k01035089</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">6</td>\n",
    "    <td style = \"text-align: left\">Martin</td>\n",
    "    <td style = \"text-align: left\">Zwiffl</td>\n",
    "    <td style = \"text-align: left\">k11910668</td>\n",
    "  </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
    "<i>Describe how to install additional packages, if you have some, here</i><br><br>\n",
    "\n",
    "<b>No additional libaries are needed/installed</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
    "<i>Place for additional comments and argumentation</i><br><br>\n",
    "We implemented the stopping criterion based on the hints given in the second phase PDF. Description of the funciton can be found in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for stopping criterium\n",
    "def stopping_criteria(x_start, x_curr, x_prev, x_after_start, fn, gradient_fn, iteration, tolerance=1e-7, max_steps=1e6):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_start: Start point coordinates. Postion on which the algorithm should start\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_curr: Current point coordinates. Postion on which the algorithm is currently\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_prev: Previous point coordinates. Postion on which the algorithm is was in the step before\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_after_start: Fist calculated point coordinates. Postion on which the algorithm was after the first step\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    iteration: Number of the current iteration the algorithm is in. Starts with 0. Is used to check max_steps\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    tolerance: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    max_steps: Number of the current iteration the algorithm is in. Starts with 0. Is used to check max_steps\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - this function checks if the gradient of the current point is below the tolerance\n",
    "    - Additionally it checks if the algo exceeds the allowed max steps\n",
    "    - It is also checked, if the gradient of the current step is smaller than the gradient in the start point\n",
    "      times our tolerance. In our case we noticed by testing, that the algorithm stops a bit too soon, therefore\n",
    "      we introduced a correction factor, which additionally lowers the error bound. Again --> Payoff runtime/acc\n",
    "    - As mentioned in the second phase pdf, we can make the stopping crit relative, by comparing it to the \n",
    "      tolerance multiplied with the difference from the start to the first point. this can be done for f(x) and x.\n",
    "    \"\"\"\n",
    "    correction_factor = 1e-3 # factor to adjust error bound and gain more precicsion (later stopping). Explained above.\n",
    "    \n",
    "    if np.linalg.norm(gradient_fn(x_curr)) <= tolerance:\n",
    "        print(\"*** GRADIENT BELOW TOLERANCE ***\")\n",
    "        return True\n",
    "    \n",
    "    if iteration >= max_steps:\n",
    "        print(\"*** MAX STEPS EXCEEDED ***\")\n",
    "        return True\n",
    "    \n",
    "    if np.linalg.norm(gradient_fn(x_curr)) <= tolerance * np.linalg.norm(gradient_fn(x_start)) * correction_factor:\n",
    "        print(\"*** GRADIENT SMALLER THAN GRADIENT OF FIRST STEP * TOLERANCE ***\")\n",
    "        return True\n",
    "    \n",
    "    if x_prev is not None and x_after_start is not None:\n",
    "        x_diff_first_step = np.linalg.norm(x_after_start - x_start)\n",
    "        y_diff_first_step = np.abs(fn(x_after_start) - fn(x_start))\n",
    "        \n",
    "        if np.abs(fn(x_curr) - fn(x_prev)) <= tolerance * y_diff_first_step * correction_factor:\n",
    "            print(\"*** Y DIFFERENCE SMALLER THAN ADJUSTED TOLERANCE ***\")\n",
    "            return True\n",
    "        \n",
    "        if np.linalg.norm(x_curr - x_prev) <= tolerance * x_diff_first_step * correction_factor:\n",
    "            print(\"*** X DIFFERENCE SMALLER THAN ADJUSTED TOLERANCE ***\")\n",
    "            return True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.</i><br><br>\n",
    "    \n",
    "In comparison to gradient descent, Newton's method is affine invariant. Given $f$, nonsingular $A \\in \\mathbb{R}^{n \\times n}$. Let $x=A y$, and $g(y)=f(A y)$. Newton steps on $g$ are: \n",
    "\n",
    "\\begin{aligned}\n",
    "y^{+} &=y-\\left(\\nabla^{2} g(y)\\right)^{-1} \\nabla g(y) \\\n",
    "&=y-\\left(A^{T} \\nabla^{2} f(A y) A\\right)^{-1} A^{T} \\nabla f(A y) \\\n",
    "&=y-A^{-1}\\left(\\nabla^{2} f(A y)\\right)^{-1} \\nabla f(A y)\n",
    "\\end{aligned}\n",
    "\n",
    "Hence\n",
    "$$\n",
    "A y^{+}=A y-\\left(\\nabla^{2} f(A y)\\right)^{-1} \\nabla f(A y)\n",
    "$$\n",
    "i.e.\n",
    "$$\n",
    "x^{+}=x-\\left(\\nabla^{2} f(x)\\right)^{-1} f(x)\n",
    "$$\n",
    "    \n",
    "So progress is independent of problem scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.</i>\n",
    "<br><br>\n",
    "Stability is a really important topic for those type of algorithms, because if the stability is not given, it is not ensured that we even reach our goal, because the algorithm could do basically anything and even crash.<br>\n",
    "Therefore it is really important to take some measures to ensure that the algorithm is stable and works fine.<br><br>\n",
    "\n",
    "Possible Measures:<br>\n",
    "- Avoid floating points problems and round off errors<br>\n",
    "- Scale problems if necessary<br>\n",
    "- Avoid matrix inversion<br>\n",
    "- Use optimal hyperparamters for algorithm<br>\n",
    "- Improve stopping criterion (limit max steps, verify multiple attributes)<br><br>\n",
    "\n",
    "We implemented some of those measure. We tuned our tolerance (epsilon) and the step size to avoid overshooting of the algorithm. We also use float calculation to avoid big round off errors (read more in floating point section).<br>\n",
    "We also use LU decomposition to avoid direct matrix inversion and we also improved our stopping criterion. These measure have a positive impact on the stability of our algorithm. <br>\n",
    "<br>\n",
    "There are for sure more stability improvements, but as we can see on our tests, the algorithm is performing reasonable well and therefore we did not implement more features. <br><br>\n",
    "\n",
    "<b>Below we want to show a described problem, if we would not use default float arrays for our calculation.</b><br>\n",
    "You should be able to see that the numpy result (which is correct result) is equal to the result, if we typecast the int array to an float array. If we just use the int array, we get wrong results and this destroys the stability of the aglo and makes it nearly unuseable.\n",
    "<br><b>Therefore is the useage of float a major contribution for stability </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "MATRIX INVERSION WITH AND WITHOUT FLOAT CALCULATION\n",
      "============================================================================\n",
      "INVERSE MATRIX WITHOUT PRE FLOAT TYPECASTING\n",
      "[[0.5  0.25 0.5 ]\n",
      " [0.   0.5  1.  ]\n",
      " [0.   0.   1.  ]]\n",
      "============================================================================\n",
      "INVERSE MATRIX WITH PRE FLOAT TYPECASTING\n",
      "[[-1.11022302e-16  1.00000000e+00  2.00000000e+00]\n",
      " [-1.00000000e+00  2.00000000e+00  4.00000000e+00]\n",
      " [-1.00000000e+00  2.00000000e+00  5.00000000e+00]]\n",
      "============================================================================\n",
      "INVERSE MATRIX NUMPY CALCULATION (TRUE RESULT)\n",
      "[[-1.11022302e-16  1.00000000e+00  2.00000000e+00]\n",
      " [-1.00000000e+00  2.00000000e+00  4.00000000e+00]\n",
      " [-1.00000000e+00  2.00000000e+00  5.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#your function for stabilising goes here\n",
    "def lu_deco_inverse_not_float(A):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    A: Input matrix, from which the inverse has to be calculated \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - LU Decompostion matrix factorization, creates two trinangular matrices L and U. In this case this is a bit\n",
    "      simpler. We create a permuation matrix and a matrix (a), which is a combination of L and U.\n",
    "    - Based on this matrices we are able to create the inverted version of the given matrix\n",
    "    - Float conversion is missing in this function to show the problems, which happen with no typecasting and int\n",
    "      calculation\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    a = np.copy(A)\n",
    "    p = np.eye(n)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        j = i + np.argmax(np.abs(a[i:, i]))  # maximum in column\n",
    "        if np.abs(a[i, j]) < 1e-8:\n",
    "            print(\"Matrix is singular\")\n",
    "            sys.exit()\n",
    "\n",
    "        if i != j:\n",
    "            a[[i, j]] = a[[j, i]]\n",
    "            p[[i, j]] = p[[j, i]]\n",
    "\n",
    "        for i_row in range(i + 1, n):\n",
    "            a[i_row, i] = a[i_row, i] / a[i, i]\n",
    "        for i_row in range(i + 1, n):\n",
    "            for i_col in range(i + 1, n):\n",
    "                a[i_row, i_col] = a[i_row, i_col] - np.outer(a[i_row, i], a[i, i_col])\n",
    "\n",
    "    a_inv = np.copy(np.dot(p, np.eye(n)))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            a_inv[i] -= np.dot(a[i, j], a_inv[j])  # Forward substitution\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        for j in range(i + 1, n):\n",
    "            a_inv[i] = a_inv[i] - np.dot(a[i, j], a_inv[j])  # Backward substitution\n",
    "        a_inv[i] = a_inv[i] / a[i, i]\n",
    "\n",
    "    return a_inv\n",
    "\n",
    "def lu_deco_inverse_float(A):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    A: Input matrix, from which the inverse has to be calculated \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - LU Decompostion matrix factorization, creates two trinangular matrices L and U. In this case this is a bit\n",
    "      simpler. We create a permuation matrix and a matrix (a), which is a combination of L and U.\n",
    "    - Based on this matrices we are able to create the inverted version of the given matrix\n",
    "    - Float conversion is done here to get the correct results (no round of error/auto typecasting problem)\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    a = np.copy(A.astype(float))\n",
    "    p = np.eye(n)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        j = i + np.argmax(np.abs(a[i:, i]))  # maximum in column\n",
    "        if np.abs(a[i, j]) < 1e-8:\n",
    "            print(\"Matrix is singular\")\n",
    "            sys.exit()\n",
    "\n",
    "        if i != j:\n",
    "            a[[i, j]] = a[[j, i]]\n",
    "            p[[i, j]] = p[[j, i]]\n",
    "\n",
    "        for i_row in range(i + 1, n):\n",
    "            a[i_row, i] = a[i_row, i] / a[i, i]\n",
    "        for i_row in range(i + 1, n):\n",
    "            for i_col in range(i + 1, n):\n",
    "                a[i_row, i_col] = a[i_row, i_col] - np.outer(a[i_row, i], a[i, i_col])\n",
    "\n",
    "    a_inv = np.copy(np.dot(p, np.eye(n)))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            a_inv[i] -= np.dot(a[i, j], a_inv[j])  # Forward substitution\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        for j in range(i + 1, n):\n",
    "            a_inv[i] = a_inv[i] - np.dot(a[i, j], a_inv[j])  # Backward substitution\n",
    "        a_inv[i] = a_inv[i] / a[i, i]\n",
    "\n",
    "    return a_inv\n",
    "\n",
    "test_mat = np.array([[2,-1,0], [1,2,-2], [0,-1,1]]) # matrix to test scenario on. Can be chosen abitrarly\n",
    "\n",
    "print(\"============================================================================\")\n",
    "print(\"MATRIX INVERSION WITH AND WITHOUT FLOAT CALCULATION\")\n",
    "print(\"============================================================================\")\n",
    "print(\"INVERSE MATRIX WITHOUT PRE FLOAT TYPECASTING\")\n",
    "print(lu_deco_inverse_not_float(test_mat))\n",
    "print(\"============================================================================\")\n",
    "print(\"INVERSE MATRIX WITH PRE FLOAT TYPECASTING\")\n",
    "print(lu_deco_inverse_float(test_mat))\n",
    "print(\"============================================================================\")\n",
    "print(\"INVERSE MATRIX NUMPY CALCULATION (TRUE RESULT)\")\n",
    "print(np.linalg.inv(test_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
    "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.</i>\n",
    "<br><br>\n",
    "In our algorithm and calculations we only use floating point numbers. There is always the risk of roundoff errors and therefore loss in accuracy, which leads to increased error rates.<br>\n",
    "We are fully aware of this problem and therefore suggest the following options:<br>\n",
    "- Do every calculation in float, in order to avoid typecasting and high round of errors<br>\n",
    "- Increase precision of float (assign more memory (bits)) to the variables and reduce error (increased memory usage + computaton time)<br>\n",
    "- We could also use a special python libary for high precision calculation. Same as before, this increases memory usage and computation time.<br><br>\n",
    "\n",
    "As we can see, there are some measure we can take, in order to avoid round off errors. The most important measure is the first one. If we calculate everything in float, the round off error is small enough <br>\n",
    "and as we can see, based on our tests below, we do not have any problems, therefore this measure seems to be enough. If we really have the need for super high precision calculation we have to either increase the<br>\n",
    "memory size of float or use an external libary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
    "<i>Place for additional comments and argumentation</i><br><br>\n",
    "    \n",
    "The algorithm of choice for inverting matrices is based on the approach from the book <b> Numerical Optimization</b> written by <b><i> Jorge Nocedal, Stephen J. Wright</i></b> and the calculator of TU Graz (http://lampx.tugraz.at/~hadley/num/ch2/2.3.php).<br>\n",
    "We do not directly decompose the matrix into L and U, because this not really needed in our case, since we reuse the LU matrix and do not use the two parts separate. <i>a_inv</i> is basically the the LU matrix, which is than used to create inverse.\n",
    "<br><br><b>IMPORTANT:</b> This is also mentioned in the floating point/error section. If the matrix is parsed with int values, python has to do typecasting, because of the divison, which is done in this function. As far as our testing goes, typecasting leads<br>\n",
    "to an major error in calculation and leads to wrong inverse matrices. This is not only the case for this function, but also in some other cases we encountered this problem. For more details check the floating point/stability section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for invertion goes here\n",
    "def lu_deco_inverse(A):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    iteration: Number of the current iteration the algorithm is in. Starts with 0. Is used to check max_steps\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    tolerance: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    max_steps: Number of the current iteration the algorithm is in. Starts with 0. Is used to check max_steps\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - this function checks if the gradient of the current point is below the tolerance\n",
    "    - Additionally it checks if the algo exceeds the allowed max steps\n",
    "    - It is also checked, if the gradient of the current step is smaller than the gradient in the start point\n",
    "      times our tolerance. In our case we noticed by testing, that the algorithm stops a bit too soon, therefore\n",
    "      we introduced a correction factor, which additionally lowers the error bound. Again --> Payoff runtime/acc\n",
    "    - As mentioned in the second phase pdf, we can make the stopping crit relative, by comparing it to the \n",
    "      tolerance multiplied with the difference from the start to the first point. this can be done for f(x) and x.\n",
    "\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - LU Decompostion matrix factorization, creates two trinangular matrices L and U. In this case this is a bit\n",
    "      simpler. We create a permuation matrix and a matrix (a), which is a combination of L and U.\n",
    "    - Based on this matrices we are able to create the inverted version of the given matrix\n",
    "    - Float conversion is done here to get the correct results (no round of error/auto typecasting problem)\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    a = np.copy(A.astype(float))\n",
    "    p = np.eye(n)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        j = i + np.argmax(np.abs(a[i:, i]))  # maximum in column\n",
    "        if np.abs(a[i, j]) < 1e-8:\n",
    "            print(\"Matrix is singular\")\n",
    "            sys.exit()\n",
    "\n",
    "        if i != j:\n",
    "            a[[i, j]] = a[[j, i]]\n",
    "            p[[i, j]] = p[[j, i]]\n",
    "\n",
    "        for i_row in range(i + 1, n):\n",
    "            a[i_row, i] = a[i_row, i] / a[i, i]\n",
    "        for i_row in range(i + 1, n):\n",
    "            for i_col in range(i + 1, n):\n",
    "                a[i_row, i_col] = a[i_row, i_col] - np.outer(a[i_row, i], a[i, i_col])\n",
    "\n",
    "    a_inv = np.copy(np.dot(p, np.eye(n)))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            a_inv[i] -= np.dot(a[i, j], a_inv[j])  # Forward substitution\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        for j in range(i + 1, n):\n",
    "            a_inv[i] = a_inv[i] - np.dot(a[i, j], a_inv[j])  # Backward substitution\n",
    "        a_inv[i] = a_inv[i] / a[i, i]\n",
    "\n",
    "    return a_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
    "<i>Place for additional comments and argumentation</i>\n",
    "<br><br>  \n",
    "As suggested, we now approximate derivative and hessians. We did this by using the trivial formulas of section 8.1 (central- difference). <br>\n",
    "4 wrapper functions were created, because the different shapes, so we can use the aproximated derivatives, gradients, second_derivatives and hessians just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for gradient approximation goes here\n",
    "def differential_quotient_helper(fn, x, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x: point, which should be taken into account. Mostly used for the shape to recreate correct der/grad shape\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Algorithm, which approximates numericaly the gradient of a given function f\n",
    "    \"\"\"\n",
    "    res=[]\n",
    "    x = x.astype(float)\n",
    "    x=x.reshape(x.shape[1] if len(x.shape) > 1 and x.shape[1] > x.shape[0] else x.shape[0])\n",
    "    for idx in range(len(x)):\n",
    "        direction = np.zeros(len(x))\n",
    "        direction[idx] = 1\n",
    "        part_derivative = (fn(x+epsilon*direction)-fn(x-epsilon*direction))/(2*epsilon)\n",
    "        res.append(part_derivative)\n",
    "    return np.array(res)\n",
    "\n",
    "def grad_wrapper(fn,  epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Wrapper, which returns the useable gradient function of a given problem function fn\n",
    "    \"\"\"\n",
    "    return lambda x: differential_quotient_helper(fn, x,  epsilon)\n",
    "\n",
    "def hessian_wrapper(fn, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Wrapper, which returns the useable hessian function of a given problem function fn\n",
    "    \"\"\"\n",
    "    return lambda x: grad_wrapper(grad_wrapper(fn, epsilon), epsilon)(x).reshape(len(x),len(x))\n",
    "\n",
    "def derivative_wrapper(fn, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Wrapper, which returns the useable derivaitve function of a given problem function fn\n",
    "    \"\"\"\n",
    "    return lambda x: (fn(x+epsilon)-fn(x-epsilon))/(2*epsilon)\n",
    "\n",
    "def second_derivative_wrapper(fn, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Wrapper, which returns the useable second derivative function of a given problem function fn\n",
    "    \"\"\"\n",
    "    return lambda x: derivative_wrapper(derivative_wrapper(fn, epsilon), epsilon)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def newton_multi(x_start, fn, gradient_fn, hessian_fn, epsilon=1e-5,\n",
    "         activate_new_stop_crit=False,\n",
    "         activate_pre_scaling=False,\n",
    "         activate_alt_gradient=False,\n",
    "         activate_new_matrix_inversion=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_start: Start point coordinates. Postion on which the algorithm should start\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    hessian_fn: function, which is the hessian of the problem. \n",
    "                 One can parse coordinates to this function to get the hessian.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_new_stop_crit: Boolean Flag, which de/activates the usage of the additinal stopping criterations.\n",
    "                            If activated, the implemented stopping crit function is called and all features\n",
    "                            get checked.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_pre_scaling: Boolean Flag, to activate pre scaling of the problem. In this case, we have not \n",
    "                          implemented pre scaling.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_alt_gradient: Boolean Flag, to activate alternative gradient calculation. This calls the gradient\n",
    "                           wrapper and approximates the gradient function.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_new_matrix_inversion: Boolean Flag, to activate alternative matrix inversion. This calls the LU\n",
    "                                   decomposition function, but is not needed in this algo since nothing needs to\n",
    "                                   be inversed.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - newton method for quadractic multi variate examples\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    xk = x_start\n",
    "    prev_x = None\n",
    "    x_step_1 = None \n",
    "    \n",
    "    if activate_alt_gradient:\n",
    "        gradient_fn = grad_wrapper(fn)\n",
    "        hessian_fn = hessian_wrapper(fn)\n",
    "    \n",
    "    while True:\n",
    "        grad = gradient_fn(xk)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "        if activate_new_stop_crit and stopping_criteria(x_start, xk, prev_x, x_step_1, fn, gradient_fn, i):\n",
    "            break\n",
    "\n",
    "        elif not activate_new_stop_crit and grad_norm <= epsilon:\n",
    "            break\n",
    "        \n",
    "        prev_x = xk\n",
    "        \n",
    "        if not activate_new_matrix_inversion:\n",
    "            xk=xk - (np.linalg.inv(hessian_fn(xk))@grad)\n",
    "        elif activate_new_matrix_inversion:\n",
    "            xk=xk - (lu_deco_inverse(hessian_fn(xk))@grad)\n",
    "            \n",
    "        if i == 0:\n",
    "            x_step_1 = xk\n",
    "            \n",
    "        i+= 1\n",
    "        \n",
    "    print(\"performed \" + str(i+1) + \" iterations.\")\n",
    "    return xk\n",
    "\n",
    "def newton_uni(x_start, fn, derivative_fn, second_derivative_fn, epsilon=1e-6,\n",
    "         activate_new_stop_crit=False,\n",
    "         activate_pre_scaling=False,\n",
    "         activate_alt_gradient=False,\n",
    "         activate_new_matrix_inversion=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_start: Start point coordinates. Postion on which the algorithm should start\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_new_stop_crit: Boolean Flag, which de/activates the usage of the additinal stopping criterations.\n",
    "                            If activated, the implemented stopping crit function is called and all features\n",
    "                            get checked.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_pre_scaling: Boolean Flag, to activate pre scaling of the problem. In this case, we have not \n",
    "                          implemented pre scaling.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_alt_gradient: Boolean Flag, to activate alternative gradient calculation. This calls the gradient\n",
    "                           wrapper and approximates the gradient function.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_new_matrix_inversion: Boolean Flag, to activate alternative matrix inversion. This calls the LU\n",
    "                                   decomposition function, but is not needed in this algo since nothing needs to\n",
    "                                   be inversed.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - newton method for quadractic multi variate examples\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    xk = x_start\n",
    "    prev_x = None\n",
    "    x_step_1 = None\n",
    "    \n",
    "    if activate_alt_gradient:\n",
    "        derivative_fn = derivative_wrapper(fn)\n",
    "        second_derivative_fn = second_derivative_wrapper(fn)\n",
    "    \n",
    "    while True:\n",
    "        der = derivative_fn(xk)\n",
    "        \n",
    "\n",
    "        if activate_new_stop_crit and stopping_criteria(x_start, xk, prev_x, x_step_1, fn, derivative_fn, i):\n",
    "            break\n",
    "\n",
    "        elif not activate_new_stop_crit and abs(der) <= epsilon:\n",
    "            break\n",
    "\n",
    "        prev_x = xk\n",
    "        xk=xk - der/second_derivative_fn(xk)\n",
    "        \n",
    "        if i == 0:\n",
    "            x_step_1 = xk\n",
    "        \n",
    "        i+= 1\n",
    "    print(\"performed \" + str(i+1) + \" iterations.\")\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def get_random_A_b_minimizer(seed):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    seed: Seed is an int value, which is used to create reproducable random values.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - 10x10 random matrix is produced by this function, which is the base of the problem\n",
    "    - 10x1 random vector is produced, which is also the minimizer of the function and the goal of the algo\n",
    "    - Function uses thoes variables to create a problem function, which can be solved by the algo\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    A=np.random.randint(0, 2, (10, 10))\n",
    "    A=A@A.T\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    minimizer=np.random.randint(0, 10, (10, 1))\n",
    "\n",
    "    b=A@minimizer\n",
    "\n",
    "    return A, b, minimizer\n",
    "\n",
    "# Created problem instances, which are used in the functions below, to provide a useable set of prolems for the algo, A and b correspond to function depended values and Minimizer is the solution to the problem\n",
    "A_1, b_1, minimizer_1 = get_random_A_b_minimizer(0)\n",
    "A_2, b_2, minimizer_2 = get_random_A_b_minimizer(2)\n",
    "A_3, b_3, minimizer_3 = get_random_A_b_minimizer(3)\n",
    "A_4, b_4, minimizer_4 = get_random_A_b_minimizer(12)\n",
    "A_5, b_5, minimizer_5 = get_random_A_b_minimizer(15)\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# problem_x_q(x) --> is the problem function f, which is used by algorithm.\n",
    "# grad_problem_X_q(x) --> as the name implies, this function is the gradient of the problem function f (problem_X_q)\n",
    "# shessian_problem_X_q(x) --> as the name implies, this function is the hessian of the problem function f (problem_X)\n",
    "# in all cases you can parse x coordinates to the functions in order to receive the corresponding y axis values.\n",
    "#############################################################################################################################################################\n",
    "# Problem 1\n",
    "\n",
    "def problem_1_q(x):\n",
    "    return (x.T@A_1@x)/2-b_1.T@x\n",
    "\n",
    "def grad_problem_1_q(x):\n",
    "    return np.dot(A_1,x)-b_1\n",
    "\n",
    "def hessian_problem_1_q(x):\n",
    "    return A_1\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 2\n",
    "\n",
    "def problem_2_q(x):\n",
    "    return (x.T@A_2@x)/2-b_2.T@x\n",
    "\n",
    "def grad_problem_2_q(x):\n",
    "    return A_2@x-b_2\n",
    "\n",
    "def hessian_problem_2_q(x):\n",
    "    return A_2\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 3\n",
    "\n",
    "def problem_3_q(x):\n",
    "    return (x.T@A_3@x)/2-b_3.T@x\n",
    "\n",
    "def grad_problem_3_q(x):\n",
    "    return A_3@x-b_3\n",
    "\n",
    "def hessian_problem_3_q(x):\n",
    "    return A_3\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 4\n",
    "\n",
    "def problem_4_q(x):\n",
    "    return (x.T@A_4@x)/2-b_4.T@x\n",
    "\n",
    "def grad_problem_4_q(x):\n",
    "    return A_4@x-b_4\n",
    "\n",
    "def hessian_problem_4_q(x):\n",
    "    return A_4\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 5\n",
    "\n",
    "def problem_5_q(x):\n",
    "    return (x.T@A_5@x)/2-b_5.T@x\n",
    "\n",
    "def grad_problem_5_q(x):\n",
    "    return A_5@x-b_5\n",
    "\n",
    "def hessian_problem_5_q(x):\n",
    "    return A_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><b>Note:</b> After every test print out the resulsts. \n",
    "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
    "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_0: numpy 1D array, corresponds to initial point\n",
    "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
    "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
    "                                      **args\n",
    "       Function f returns a scalar output.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
    "                                         function f,\n",
    "                                         args (which are submitted, because you might need\n",
    "                                              to call f(x,**args) inside your gradient function implementation). \n",
    "          Function grad approximates gradient at given point and returns a 1d np array.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    args: dictionary, additional (except of x) arguments to function f\n",
    "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Initial x is :\\t\\t{x_0}')\n",
    "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
    "    print(f'Approximated x is :\\t{x_appr}')\n",
    "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
    "    f_opt = f(x_optimal)\n",
    "    f_appr = f(x_appr)\n",
    "    print(f'Function value in optimal point:\\t{f_opt}')\n",
    "    print(f'Function value in approximated point:   {f_appr}')\n",
    "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
    "    print(f'Gradient approximation in optimal point is:\\n{grad(x_optimal)}\\n')\n",
    "    grad_appr = grad(x_appr)\n",
    "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
    "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "DEFAULT TESTS WITHOUT ANY ADDITIONAL FEATURES\n",
      "============================================================================\n",
      "**** PROBLEM 1 ****\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[5.00000000e+00]\n",
      " [7.10542736e-15]\n",
      " [3.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [7.00000000e+00]\n",
      " [9.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [5.00000000e+00]\n",
      " [2.00000000e+00]\n",
      " [4.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 8.52651283e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 5.68434189e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 2.84217094e-14]\n",
      " [ 5.68434189e-14]\n",
      " [ 0.00000000e+00]\n",
      " [ 2.84217094e-14]\n",
      " [ 0.00000000e+00]\n",
      " [-2.84217094e-14]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 2 ****\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [8]\n",
      " [6]\n",
      " [2]\n",
      " [8]\n",
      " [7]\n",
      " [2]\n",
      " [1]\n",
      " [5]\n",
      " [4]]\n",
      "Approximated x is :\t[[8.]\n",
      " [8.]\n",
      " [6.]\n",
      " [2.]\n",
      " [8.]\n",
      " [7.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3230.]]\n",
      "Function value in approximated point:   [[-3230.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-3.97903932e-13]\n",
      " [-5.96855898e-13]\n",
      " [-4.12114787e-13]\n",
      " [-3.97903932e-13]\n",
      " [-3.69482223e-13]\n",
      " [-3.69482223e-13]\n",
      " [-3.97903932e-13]\n",
      " [-4.54747351e-13]\n",
      " [-2.13162821e-13]\n",
      " [-1.84741111e-13]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 3 ****\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [9]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [5]\n",
      " [3]\n",
      " [9]\n",
      " [9]]\n",
      "Approximated x is :\t[[8.00000000e+00]\n",
      " [9.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [8.00000000e+00]\n",
      " [8.00000000e+00]\n",
      " [1.70530257e-13]\n",
      " [5.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [9.00000000e+00]\n",
      " [9.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-5567.5]]\n",
      "Function value in approximated point:   [[-5567.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-1.36424205e-12]\n",
      " [-5.96855898e-13]\n",
      " [-1.98951966e-13]\n",
      " [-1.25055521e-12]\n",
      " [-1.27897692e-12]\n",
      " [-2.84217094e-13]\n",
      " [-6.25277607e-13]\n",
      " [-1.67688086e-12]\n",
      " [-1.30739863e-12]\n",
      " [-6.25277607e-13]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 4 ****\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [5]]\n",
      "Approximated x is :\t[[ 6.00000000e+00]\n",
      " [ 1.00000000e+00]\n",
      " [ 2.00000000e+00]\n",
      " [ 3.00000000e+00]\n",
      " [ 3.00000000e+00]\n",
      " [-3.12638804e-13]\n",
      " [ 6.00000000e+00]\n",
      " [ 1.00000000e+00]\n",
      " [ 4.00000000e+00]\n",
      " [ 5.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-1475.5]]\n",
      "Function value in approximated point:   [[-1475.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[2.35900188e-12]\n",
      " [1.83320026e-12]\n",
      " [2.00373051e-12]\n",
      " [1.20792265e-12]\n",
      " [1.06581410e-12]\n",
      " [1.43529633e-12]\n",
      " [1.74793513e-12]\n",
      " [1.50635060e-12]\n",
      " [1.33582034e-12]\n",
      " [1.10844667e-12]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 5 ****\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [5]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [5]\n",
      " [6]\n",
      " [1]\n",
      " [7]]\n",
      "Approximated x is :\t[[ 8.00000000e+00]\n",
      " [ 5.00000000e+00]\n",
      " [ 5.00000000e+00]\n",
      " [ 7.00000000e+00]\n",
      " [-1.42108547e-14]\n",
      " [ 7.00000000e+00]\n",
      " [ 5.00000000e+00]\n",
      " [ 6.00000000e+00]\n",
      " [ 1.00000000e+00]\n",
      " [ 7.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4121.5]]\n",
      "Function value in approximated point:   [[-4121.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-2.84217094e-13]\n",
      " [-3.41060513e-13]\n",
      " [-2.27373675e-13]\n",
      " [-2.84217094e-13]\n",
      " [-2.55795385e-13]\n",
      " [-9.94759830e-14]\n",
      " [-1.98951966e-13]\n",
      " [-2.84217094e-13]\n",
      " [-1.70530257e-13]\n",
      " [-2.27373675e-13]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "print(\"============================================================================\")\n",
    "print(\"DEFAULT TESTS WITHOUT ANY ADDITIONAL FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "print(\"**** PROBLEM 1 ****\")\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 2 ****\")\n",
    "x_hat=newton_multi(init, problem_2_q, grad_problem_2_q, hessian_problem_2_q)\n",
    "final_printout(init, minimizer_2, x_hat, problem_2_q, grad_problem_2_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 3 ****\")\n",
    "x_hat=newton_multi(init, problem_3_q, grad_problem_3_q, hessian_problem_3_q)\n",
    "final_printout(init, minimizer_3, x_hat, problem_3_q, grad_problem_3_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 4 ****\")\n",
    "x_hat=newton_multi(init, problem_4_q, grad_problem_4_q, hessian_problem_4_q)\n",
    "final_printout(init, minimizer_4, x_hat, problem_4_q, grad_problem_4_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 5 ****\")\n",
    "x_hat=newton_multi(init, problem_5_q, grad_problem_5_q, hessian_problem_5_q)\n",
    "final_printout(init, minimizer_5, x_hat, problem_5_q, grad_problem_5_q, {}, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED ADVANCED STOPPING CRITERION FOR PROBLEM 1\n",
      "============================================================================\n",
      "*** GRADIENT BELOW TOLERANCE ***\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[5.00000000e+00]\n",
      " [7.10542736e-15]\n",
      " [3.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [7.00000000e+00]\n",
      " [9.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [5.00000000e+00]\n",
      " [2.00000000e+00]\n",
      " [4.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 8.52651283e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 5.68434189e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 2.84217094e-14]\n",
      " [ 5.68434189e-14]\n",
      " [ 0.00000000e+00]\n",
      " [ 2.84217094e-14]\n",
      " [ 0.00000000e+00]\n",
      " [-2.84217094e-14]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL STOPPING CRITERION FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[5.00000000e+00]\n",
      " [7.10542736e-15]\n",
      " [3.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [7.00000000e+00]\n",
      " [9.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [5.00000000e+00]\n",
      " [2.00000000e+00]\n",
      " [4.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 8.52651283e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 5.68434189e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 2.84217094e-14]\n",
      " [ 5.68434189e-14]\n",
      " [ 0.00000000e+00]\n",
      " [ 2.84217094e-14]\n",
      " [ 0.00000000e+00]\n",
      " [-2.84217094e-14]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED ADVANCED STOPPING CRITERION FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q, activate_new_stop_crit=True)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL STOPPING CRITERION FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q, activate_new_stop_crit=False)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED APPROXIMATED GRADIENT FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 3 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99998655e+00]\n",
      " [-9.46895490e-06]\n",
      " [ 2.99999595e+00]\n",
      " [ 3.00002107e+00]\n",
      " [ 7.00000539e+00]\n",
      " [ 9.00001754e+00]\n",
      " [ 2.99997900e+00]\n",
      " [ 5.00000566e+00]\n",
      " [ 1.99998676e+00]\n",
      " [ 4.00000363e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-2.04930700e-07]\n",
      " [ 1.39831421e-07]\n",
      " [-1.66820826e-07]\n",
      " [ 3.18845196e-07]\n",
      " [ 2.83091879e-07]\n",
      " [-3.39930921e-07]\n",
      " [-4.31410569e-08]\n",
      " [ 2.63258130e-07]\n",
      " [-1.08283530e-07]\n",
      " [ 3.14305368e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL GRADIENT FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[5.00000000e+00]\n",
      " [7.10542736e-15]\n",
      " [3.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [7.00000000e+00]\n",
      " [9.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [5.00000000e+00]\n",
      " [2.00000000e+00]\n",
      " [4.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 8.52651283e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 5.68434189e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 2.84217094e-14]\n",
      " [ 5.68434189e-14]\n",
      " [ 0.00000000e+00]\n",
      " [ 2.84217094e-14]\n",
      " [ 0.00000000e+00]\n",
      " [-2.84217094e-14]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED APPROXIMATED GRADIENT FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q, activate_alt_gradient=True)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL GRADIENT FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q, activate_alt_gradient=False)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED ALTERNATIVE MATRIX INVERSION FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 5.00000000e+00]\n",
      " [-6.39488462e-14]\n",
      " [ 3.00000000e+00]\n",
      " [ 3.00000000e+00]\n",
      " [ 7.00000000e+00]\n",
      " [ 9.00000000e+00]\n",
      " [ 3.00000000e+00]\n",
      " [ 5.00000000e+00]\n",
      " [ 2.00000000e+00]\n",
      " [ 4.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 0.00000000e+00]\n",
      " [-1.27897692e-13]\n",
      " [ 2.84217094e-14]\n",
      " [ 8.52651283e-14]\n",
      " [-1.13686838e-13]\n",
      " [-1.42108547e-13]\n",
      " [-1.42108547e-14]\n",
      " [-4.26325641e-14]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL MATRIX INVERSION FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[5.00000000e+00]\n",
      " [7.10542736e-15]\n",
      " [3.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [7.00000000e+00]\n",
      " [9.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [5.00000000e+00]\n",
      " [2.00000000e+00]\n",
      " [4.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 8.52651283e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 5.68434189e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 2.84217094e-14]\n",
      " [ 5.68434189e-14]\n",
      " [ 0.00000000e+00]\n",
      " [ 2.84217094e-14]\n",
      " [ 0.00000000e+00]\n",
      " [-2.84217094e-14]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED ALTERNATIVE MATRIX INVERSION FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q, activate_new_matrix_inversion=True)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL MATRIX INVERSION FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q, activate_new_matrix_inversion=False)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED APPROXIMATED GRADIENT AND STOPPING CRITERION AND ALT MATRIX INVERSION FOR PROBLEM 1\n",
      "============================================================================\n",
      "*** GRADIENT BELOW TOLERANCE ***\n",
      "performed 4 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[5.00000003e+00]\n",
      " [1.99577286e-08]\n",
      " [3.00000001e+00]\n",
      " [2.99999995e+00]\n",
      " [6.99999999e+00]\n",
      " [8.99999996e+00]\n",
      " [3.00000005e+00]\n",
      " [4.99999999e+00]\n",
      " [2.00000003e+00]\n",
      " [3.99999999e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-1.33260869e-09]\n",
      " [ 1.21542598e-09]\n",
      " [-2.20140350e-09]\n",
      " [ 1.62742708e-10]\n",
      " [-1.35088385e-09]\n",
      " [ 2.39140263e-09]\n",
      " [ 3.85534804e-09]\n",
      " [ 1.71189640e-09]\n",
      " [-1.70899739e-10]\n",
      " [-1.36748213e-09]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL NEWTON FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 2 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[5.00000000e+00]\n",
      " [7.10542736e-15]\n",
      " [3.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [7.00000000e+00]\n",
      " [9.00000000e+00]\n",
      " [3.00000000e+00]\n",
      " [5.00000000e+00]\n",
      " [2.00000000e+00]\n",
      " [4.00000000e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 8.52651283e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 5.68434189e-14]\n",
      " [-5.68434189e-14]\n",
      " [ 2.84217094e-14]\n",
      " [ 5.68434189e-14]\n",
      " [ 0.00000000e+00]\n",
      " [ 2.84217094e-14]\n",
      " [ 0.00000000e+00]\n",
      " [-2.84217094e-14]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED APPROXIMATED GRADIENT AND STOPPING CRITERION AND ALT MATRIX INVERSION FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q, \n",
    "                   activate_new_matrix_inversion=True, \n",
    "                   activate_alt_gradient=True,\n",
    "                   activate_new_stop_crit=True\n",
    "                  )\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL NEWTON FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "x_hat=newton_multi(init, problem_1_q, grad_problem_1_q, hessian_problem_1_q, \n",
    "                                activate_new_matrix_inversion=False, \n",
    "                                activate_alt_gradient=False,\n",
    "                                activate_new_stop_crit=False\n",
    "                            )\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>\n",
    "\n",
    "### ANALYSIS:\n",
    "\n",
    "Our Newton Algorithm is able to find all global minimizers, which is already really good.<br>\n",
    "The algorithm is really fast and stable, therefore our additional implemented features are not that important. We can barley see an imporvement in performance.<br>\n",
    "\n",
    "#### Stopping Criterion:\n",
    "\n",
    "With the advanced stopping criterion we are able to safe some iterations, depending on the example, but it is not really necessary, since the Newton Method is already really fast.<br>\n",
    "The results do not get worse by this feature, therefore we can still use it and if we encounter a special example, where the advanced stopping criterion is useful, we still benefit from it.\n",
    "\n",
    "#### Approximated Gradient:\n",
    "\n",
    "As we can see above, the approxmation algorithm works perfectly. There is only a small difference between the results. If the difference would be larger, <br>\n",
    "we would notice that the algorithm converges to infinity and would be able to find the minimizer. Therefore it is really important to find the right tolerance.<br>\n",
    "Again as already mentioned in the stability section, we have to find the middle way, between low error and fast execution.<br><br>\n",
    "In this case we do not really see a problem, again not really an improvement, but it also does not get worse. It is still a tradeoff, but Newton works also really well without it.\n",
    "\n",
    "#### Alternative Matrix Inversion:\n",
    "\n",
    "The normal Newton Method uses Matrix Inversion and since we know that one should not inverse matrices, because this can cause several problems, we should use an approximation. This approximation solves the mentioned problems and increases the stability of the algorithm. In the examples above, you can see the test of the new matrix inversion and you can see that it functions really well and provides equal results, compared to the built in numpy method. Our examples are not really critical, therefore it does not really make a difference, but it is important to make the algorithm future proof, for examples, which are more difficult to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "# Stationary points of each problem. Number at the end of abc_X is the problem number. abc indicates the name of the of the 3 stationary points a, b and c.\n",
    "abc_6 = np.array([1, 20, 300])\n",
    "abc_7 = np.array([4, 50, 600])\n",
    "abc_8 = np.array([7, 80, 900])\n",
    "abc_9 = np.array([1, 100, 200])\n",
    "abc_10 = np.array([2, 120, 200])\n",
    "\n",
    "\n",
    "# Function to find the global minimizer from all stationary points, which are given by the function creation\n",
    "def get_global_minimizer(problem, stat_points):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    problem: function which takes x coordinates to calculate y axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    stat_points: array of stationary points, which should be compared\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    for stat_point in np.array(stat_points).astype(float):\n",
    "        y.append([problem(stat_point), stat_point])\n",
    "    y = np.array(y)\n",
    "    return y[y[:,0].argsort()][0, 1]\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# problem_x(x) --> is the problem function f, which is used by algorithm.\n",
    "# derivative_problem_X(x) --> as the name implies, this function is the derivative of the problem function f (problem_X)\n",
    "# second_derivative_problem_X(x) --> as the name implies, this function is the second derivative of the problem function f (problem_X)\n",
    "# in all cases you can parse x coordinates to the functions in order to receive the corresponding y axis values.\n",
    "#############################################################################################################################################################\n",
    "# Problem 6\n",
    "\n",
    "def problem_6(x):\n",
    "    return -(x*(3*x**3+(-4*abc_6[2]-4*abc_6[1]-4*abc_6[0])*x**2+((6*abc_6[1]+6*abc_6[0])*abc_6[2]+6*abc_6[0]*abc_6[1])*x-12*abc_6[0]*abc_6[1]*abc_6[2]))/12\n",
    "\n",
    "def derivative_problem_6(x):\n",
    "    return (abc_6[0]-x)*(abc_6[1]-x)*(abc_6[2]-x)\n",
    "\n",
    "def second_derivative_problem_6(x):\n",
    "    return -(abc_6[1]-x)*(abc_6[2]-x)-(abc_6[0]-x)*(abc_6[2]-x)-(abc_6[0]-x)*(abc_6[1]-x)\n",
    "#############################################################################################################################################################\n",
    "# Problem 7\n",
    "\n",
    "def problem_7(x):\n",
    "    return -(x*(3*x**3+(-4*abc_7[2]-4*abc_7[1]-4*abc_7[0])*x**2+((6*abc_7[1]+6*abc_7[0])*abc_7[2]+6*abc_7[0]*abc_7[1])*x-12*abc_7[0]*abc_7[1]*abc_7[2]))/12\n",
    "\n",
    "def derivative_problem_7(x):\n",
    "    return (abc_7[0]-x)*(abc_7[1]-x)*(abc_7[2]-x)\n",
    "\n",
    "def second_derivative_problem_7(x):\n",
    "    return -(abc_7[1]-x)*(abc_7[2]-x)-(abc_7[0]-x)*(abc_7[2]-x)-(abc_7[0]-x)*(abc_7[1]-x)\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 8\n",
    "\n",
    "def problem_8(x):\n",
    "    return -(x*(3*x**3+(-4*abc_8[2]-4*abc_8[1]-4*abc_8[0])*x**2+((6*abc_8[1]+6*abc_8[0])*abc_8[2]+6*abc_8[0]*abc_8[1])*x-12*abc_8[0]*abc_8[1]*abc_8[2]))/12\n",
    "\n",
    "def derivative_problem_8(x):\n",
    "    return (abc_8[0]-x)*(abc_8[1]-x)*(abc_8[2]-x)\n",
    "\n",
    "def second_derivative_problem_8(x):\n",
    "    return -(abc_8[1]-x)*(abc_8[2]-x)-(abc_8[0]-x)*(abc_8[2]-x)-(abc_8[0]-x)*(abc_8[1]-x)\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 9\n",
    "\n",
    "def problem_9(x):\n",
    "    return -(x*(3*x**3+(-4*abc_9[2]-4*abc_9[1]-4*abc_9[0])*x**2+((6*abc_9[1]+6*abc_9[0])*abc_9[2]+6*abc_9[0]*abc_9[1])*x-12*abc_9[0]*abc_9[1]*abc_9[2]))/12\n",
    "\n",
    "def derivative_problem_9(x):\n",
    "    return (abc_9[0]-x)*(abc_9[1]-x)*(abc_9[2]-x)\n",
    "\n",
    "def second_derivative_problem_9(x):\n",
    "    return -(abc_9[1]-x)*(abc_9[2]-x)-(abc_9[0]-x)*(abc_9[2]-x)-(abc_9[0]-x)*(abc_9[1]-x)\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 10\n",
    "\n",
    "def problem_10(x):\n",
    "    return -(x*(3*x**3+(-4*abc_10[2]-4*abc_10[1]-4*abc_10[0])*x**2+((6*abc_10[1]+6*abc_10[0])*abc_10[2]+6*abc_10[0]*abc_10[1])*x-12*abc_10[0]*abc_10[1]*abc_10[2]))/12\n",
    "\n",
    "def derivative_problem_10(x):\n",
    "    return (abc_10[0]-x)*(abc_10[1]-x)*(abc_10[2]-x)\n",
    "\n",
    "def second_derivative_problem_10(x):\n",
    "    return -(abc_10[1]-x)*(abc_10[2]-x)-(abc_10[0]-x)*(abc_10[2]-x)-(abc_10[0]-x)*(abc_10[1]-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "DEFAULT TESTS WITHOUT ANY ADDITIONAL FEATURES\n",
      "============================================================================\n",
      "**** PROBLEM 6 ****\n",
      "performed 8 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-0.]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 7 ****\n",
      "performed 7 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t50.0\n",
      "Approximated x is :\t[[50.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-9062500.0\n",
      "Function value in approximated point:   [[-9062500.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-0.]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 8 ****\n",
      "performed 6 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t80.0\n",
      "Approximated x is :\t[[80.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-53824000.0\n",
      "Function value in approximated point:   [[-53824000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-0.]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 9 ****\n",
      "performed 1 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t100.0\n",
      "Approximated x is :\t[[100.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-24166666.666666668\n",
      "Function value in approximated point:   [[-24166666.66666667]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-0.]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 10 ****\n",
      "performed 4 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t120.0\n",
      "Approximated x is :\t[[120.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-38016000.0\n",
      "Function value in approximated point:   [[-38016000.00000003]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-2.80374479e-08]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "x_start = np.array([[100]], dtype=float)  # start point for all problems\n",
    "print(\"============================================================================\")\n",
    "print(\"DEFAULT TESTS WITHOUT ANY ADDITIONAL FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 6 ****\")\n",
    "x_hat = newton_uni(x_start, problem_6,  derivative_problem_6, second_derivative_problem_6)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 7 ****\")\n",
    "x_hat=newton_uni(x_start, problem_7,  derivative_problem_7, second_derivative_problem_7)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_7, abc_7), x_appr=x_hat, f=problem_7, grad=derivative_problem_7, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 8 ****\")\n",
    "x_hat=newton_uni(x_start, problem_8,  derivative_problem_8, second_derivative_problem_8)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_8, abc_8), x_appr=x_hat, f=problem_8, grad=derivative_problem_8, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 9 ****\")\n",
    "x_hat=newton_uni(x_start, problem_9,  derivative_problem_9, second_derivative_problem_9)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_9, abc_9), x_appr=x_hat, f=problem_9, grad=derivative_problem_9, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 10 ****\")\n",
    "x_hat=newton_uni(x_start, problem_10,  derivative_problem_10, second_derivative_problem_10)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_10, abc_10), x_appr=x_hat, f=problem_10, grad=derivative_problem_10, args={}, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED ADVANCED STOPPING CRITERION FOR PROBLEM 6\n",
      "============================================================================\n",
      "*** GRADIENT SMALLER THAN GRADIENT OF FIRST STEP * TOLERANCE ***\n",
      "performed 7 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.00000001]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[4.20502021e-05]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL STOPPING CRITERION FOR PROBLEM 6\n",
      "============================================================================\n",
      "performed 8 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-0.]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED ADVANCED STOPPING CRITERION FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_start = np.array([[100]], dtype=float)  # start point for all problems\n",
    "x_hat = newton_uni(x_start, problem_6,  derivative_problem_6, second_derivative_problem_6, activate_new_stop_crit=True)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL STOPPING CRITERION FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_hat = newton_uni(x_start, problem_6,  derivative_problem_6, second_derivative_problem_6)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED APPROXIMATED GRADIENT FOR PROBLEM 6\n",
      "============================================================================\n",
      "performed 8 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-1.61101653e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL GRADIENT FOR PROBLEM 6\n",
      "============================================================================\n",
      "performed 8 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-0.]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED APPROXIMATED GRADIENT FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_start = np.array([[100]], dtype=float)  # start point for all problems\n",
    "x_hat = newton_uni(x_start, problem_6,  derivative_problem_6, second_derivative_problem_6, activate_alt_gradient=True)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL GRADIENT FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_hat = newton_uni(x_start, problem_6,  derivative_problem_6, second_derivative_problem_6, activate_alt_gradient=False)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED APPROXIMATED GRADIENT AND STOPPING CRIT FOR PROBLEM 6\n",
      "============================================================================\n",
      "*** GRADIENT SMALLER THAN GRADIENT OF FIRST STEP * TOLERANCE ***\n",
      "performed 7 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.00000001]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[3.82612126e-05]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL NEWTON FOR PROBLEM 6\n",
      "============================================================================\n",
      "performed 8 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-0.]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED APPROXIMATED GRADIENT AND STOPPING CRIT FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_start = np.array([[100]], dtype=float)  # start point for all problems\n",
    "x_hat = newton_uni(x_start, problem_6,  derivative_problem_6, second_derivative_problem_6, activate_alt_gradient=True, activate_new_stop_crit=True)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL NEWTON FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_hat = newton_uni(x_start, problem_6,  derivative_problem_6, second_derivative_problem_6, activate_alt_gradient=False)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>\n",
    "\n",
    "### ANALYSIS:\n",
    "\n",
    "Our Newton Algorithm is able to find all global minimizers, which is already really good.<br>\n",
    "The algorithm is really fast and stable, therefore our additional implemented features are not that important. We can barley see an imporvement in performance.<br>\n",
    "\n",
    "#### Stopping Criterion:\n",
    "\n",
    "With the advanced stopping criterion we are able to safe some iterations, depending on the example, but it is not really necessary, since the Newton Method is already really fast.<br>\n",
    "The results do not get worse by this feature, therefore we can still use it and if we encounter a special example, where the advanced stopping criterion is useful, we still benefit from it.\n",
    "\n",
    "#### Approximated Gradient:\n",
    "\n",
    "As we can see above, the approxmation algorithm works perfectly. There is only a small difference between the results. If the difference would be larger, <br>\n",
    "we would notice that the algorithm converges to infinity and would be able to find the minimizer. Therefore it is really important to find the right tolerance.<br>\n",
    "Again as already mentioned in the stability section, we have to find the middle way, between low error and fast execution.<br><br>\n",
    "In this case we do not really see a problem, again not really an improvement, but it also does not get worse. It is still a tradeoff, but Newton works also really well without it.\n",
    "\n",
    "## Comparison:\n",
    "\n",
    "#### Steepest Descent:\n",
    "\n",
    "Needs rescaling for unbalanced problem function.<br>\n",
    "Needs many iterations >20k to find minimizer.<br>\n",
    "Does not reach lowest error bound, with advanced stopping criterion. This could be fixed, but runtime would be higher.<br>\n",
    "Stable as long problems are not too complicated (unbalanced, many local min, etc.)<br>\n",
    "\n",
    "#### Conjugate Gradient:\n",
    "\n",
    "Needs rescaling for unbalanced problem function.<br>\n",
    "Needs many iterations >20k to find minimizer.<br>\n",
    "Does not reach lowest error bound, with advanced stopping criterion. This could be fixed, but runtime would be higher. (Better than steepest descent, still not optimal)<br>\n",
    "Stable as long problems are not too complicated (unbalanced, many local min, etc.)<br>\n",
    "\n",
    "#### Newton (CURRENT):\n",
    "\n",
    "Does not need rescaling for unbalanced problem function. Newton default behaviour.<br>\n",
    "Really fast <10 iterations for our examples to find solution.<br>\n",
    "Reaches lowest error bound with and without additional features.<br>\n",
    "Stable with and without additional features, this could change for more difficult problems, but has not been tested <br>\n",
    "\n",
    "#### Quasi Netwon:\n",
    "\n",
    "\n",
    "Does not need rescaling for unbalanced problem function. Newton default behaviour.<br>\n",
    "Really fast <30 iterations for our examples to find solution.<br>\n",
    "Reaches lowest error bound with and without additional features.<br>\n",
    "Stable with and without additional features, this could change for more difficult problems, but has not been tested <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
    "Template should include sceletons for:<ul>\n",
    "    <li>custom function to optimise over </li> \n",
    "    <li>values initialisation to submit into otimising algorithm </li> \n",
    "    <li>optimiser function call</li> \n",
    "    <li>report print out call</li> </ul><br>\n",
    "Provide descriptions and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** GRADIENT BELOW TOLERANCE ***\n",
      "performed 4 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[3]\n",
      " [9]\n",
      " [4]\n",
      " [6]\n",
      " [7]\n",
      " [2]\n",
      " [0]\n",
      " [6]\n",
      " [8]\n",
      " [5]]\n",
      "Approximated x is :\t[[ 2.99999830e+00]\n",
      " [ 9.00000432e+00]\n",
      " [ 3.99999813e+00]\n",
      " [ 5.99999386e+00]\n",
      " [ 6.99999888e+00]\n",
      " [ 2.00000351e+00]\n",
      " [-6.47425291e-07]\n",
      " [ 6.00000458e+00]\n",
      " [ 8.00000856e+00]\n",
      " [ 4.99999418e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-2881.5]]\n",
      "Function value in approximated point:   [[-2881.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 9.93807703e-09]\n",
      " [ 2.24970620e-08]\n",
      " [-3.21605853e-09]\n",
      " [ 3.85455223e-10]\n",
      " [-4.27155555e-09]\n",
      " [-1.08585141e-10]\n",
      " [-3.42416229e-08]\n",
      " [-1.43780881e-08]\n",
      " [ 1.51937058e-08]\n",
      " [ 1.48570223e-09]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "==================================================================================================================================\n",
      "*** GRADIENT BELOW TOLERANCE ***\n",
      "performed 6 iterations.\n",
      "Initial x is :\t\t[[120.]]\n",
      "Optimal x is :\t\t200.0\n",
      "Approximated x is :\t[[200.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-386666666.6666667\n",
      "Function value in approximated point:   [[-3.86666667e+08]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-0.]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**** Teacher Test ****\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "** Custom Function to optimise over\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "- All problems, quadratic and non quadratic are based on the instruction from project phase 1.\n",
    "*** Quadratic problems\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "- One can easily create new quadratic problems by changing the seed of the \"get_random_A_b_minimizer\" function. This function uses the seed to create random matrices, which are than used for the problem function.\n",
    "- The three functions problem_x_q, grad_problem_x and hessian_problem_x are used by the optimizer and those functions use the return values of the get_random_A_b_minimizer functions to create the problem function.\n",
    "- Therefore one can change the seed in the mentioned function and create a new problem, which can be recreated and is still random.\n",
    "- It is also possible to create completly different problems, but the structure has to match.\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "*** Non Quadratic problems\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "- One can also create new non quadratic problems by changing the values of the \"abc_x\" array. Those are the pre defined stationary points of the problem function. By choosing different points, the problem changes.\n",
    "- The three functions problem_x, derivative_problem_x and second_derivative_problem_x are used by the optimizer and those functions use the abc_x array to create the problem function.\n",
    "- Therefore one can change the content of the abc_x array and create a new problem. \n",
    "- !!!!!!!!!!!!!!!IMPORTANT!!!!!!!!!!!!!!!!!!!!: No additional values should be entered, this means the length of this array should be 3, otherwise the solution might not match.\n",
    "- It is also possible to create completly different problems, but the structure has to match.\n",
    "\"\"\"\n",
    "\n",
    "def get_random_A_b_minimizer(seed):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    seed: Seed is an int value, which is used to create reproducable random values.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - 10x10 random matrix is produced by this function, which is the base of the problem\n",
    "    - 10x1 random vector is produced, which is also the minimizer of the function and the goal of the algo\n",
    "    - Function uses thoes variables to create a problem function, which can be solved by the algo\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    A=np.random.randint(0, 2, (10, 10))\n",
    "    A=A@A.T\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    minimizer=np.random.randint(0, 10, (10, 1))\n",
    "\n",
    "    b=A@minimizer\n",
    "\n",
    "    return A, b, minimizer\n",
    "\n",
    "# Quadratic problem\n",
    "A_6, b_6, minimizer_6 = get_random_A_b_minimizer(20)\n",
    "\n",
    "def problem_6_q(x):\n",
    "    return (x.T@A_6@x)/2-b_6.T@x\n",
    "\n",
    "def grad_problem_6_q(x):\n",
    "    return np.dot(A_6,x)-b_6\n",
    "\n",
    "def hessian_problem_6_q(x):\n",
    "    return A_6\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Non quadratic problem\n",
    "\n",
    "abc_11 = np.array([2, 200, 400])\n",
    "def problem_11(x):\n",
    "    return -(x*(3*x**3+(-4*abc_11[2]-4*abc_11[1]-4*abc_11[0])*x**2+((6*abc_11[1]+6*abc_11[0])*abc_11[2]+6*abc_11[0]*abc_11[1])*x-12*abc_11[0]*abc_11[1]*abc_11[2]))/12\n",
    "\n",
    "def derivative_problem_11(x):\n",
    "    return (abc_11[0]-x)*(abc_11[1]-x)*(abc_11[2]-x)\n",
    "\n",
    "def second_derivative_problem_11(x):\n",
    "    return -(abc_11[1]-x)*(abc_11[2]-x)-(abc_11[0]-x)*(abc_11[2]-x)-(abc_11[0]-x)*(abc_11[1]-x)\n",
    "\n",
    "\"\"\"\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "** Value Initalisation Optimizer & Report Call \n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "- Call function to optimise over and safe result\n",
    "- Optimizer function is called with \"newton_multi\" for quadratic problems and \"newton_uni\" for non quadratic.\n",
    "- The names come from the number of variables used. There is a strict differnce in our functions between gradient and derivative. It would be possible to use the same function for both problems, but \n",
    "- kept it consistent to the individual task\n",
    "- Decide on a example. \n",
    "-- There are 5 quadratic problems named problem_X_q (q for quadratic), the gradient is than named grad_problem_X_q and same for the hessian (X == Number of problem 1..5)\n",
    "-- There are also 5 non quadratic problems named problem_X, the derivative is than named derivative_problem_X and same for the second derivative (X == Number of problem 6..10)\n",
    "- We have to also define a starting point for the optimizer. All of our problems, in the quadratic case, have to in shape 10x1, since our quadratic problems have 10 variables\n",
    "- The non quadratic problems need a 1x1 starting point, since our problems have only 1 variable.\n",
    "- Additionally one has to choose the extra features. For Newton we implemented the advanced stopping criterion and the approximated gradient and Approximated Inverse (only for Multi). Scaling is not implemented, for reasons checkout the corresponding chapters.\n",
    "- For the final printout, we use the given function. There is a difference between quadratic and non quadratic problems. Our non quadratic problems have 3 stationary points and we have to call the function\n",
    "  get_global_minimizer to find the real minimizer. This has to be done for the non quadratic problems.\n",
    "- x_optimal is for the quadratic problem the variable called \"minimizer_X\", where X has to be replaced by the problem number (1..5)\n",
    "- For the non quadratic problems we have an array called \"abc_X\", where X has to be replaced by the problem number (6..10), which contains all stationary points.\n",
    "\"\"\"\n",
    "##########################################################################################################################################################################################\n",
    "# Quadratic problem call\n",
    "\n",
    "x_start = np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]]) # 10x1 start point for optimizer\n",
    "problem = problem_6_q # Choose th quadratic problem as problem to be solved by the optimizer\n",
    "gradient_problem = grad_problem_6_q # Choose 6th quadratic problem gradient to be solved by the optimizer\n",
    "second_gradient_problem = hessian_problem_6_q # Choose 6th quadratic problem hessian to be solved by the optimizer \n",
    "x_optimal = minimizer_6\n",
    "# We could choose different problems and problem gradients, but that does not make sense, therefore it is not recommended :).\n",
    "alt_gradient_flag = True # Here one can activate (True), the approximated gradient or deactivate (False) it.\n",
    "alt_stopping_crit = True # Here one can activate (True), the advanced stopping criterion or deactivate (False) it.\n",
    "alt_matrix_inv = True # Here one can activate (True), the alternative matrix inversion or deactivate (False) it.\n",
    "\n",
    "# Optimizer Function Call. All the above defined options are parsed to the optimizer call\n",
    "x_hat = newton_multi(x_start, problem,  gradient_problem, second_gradient_problem, activate_alt_gradient=alt_gradient_flag, activate_new_stop_crit=alt_stopping_crit, activate_new_matrix_inversion=alt_matrix_inv)\n",
    "# Final Printout uses the result of the optimizer + the given statements and verifys the results. Args is not used, since our algorithm and functions do not support it.\n",
    "final_printout(x_0=x_start, x_optimal=x_optimal, x_appr=x_hat, f=problem, grad=gradient_problem, args={}, tolerance=1e-3)\n",
    "print(\"==================================================================================================================================\")\n",
    "##########################################################################################################################################################################################\n",
    "# Non Quadratic problem call\n",
    "\n",
    "x_start = np.array([[120]], dtype=float) # 1x1 start point for optimizer\n",
    "problem = problem_11 # Choose 6th non quadratic problem as problem to be solved by the optimizer\n",
    "gradient_problem = derivative_problem_11 # Choose 6th non quadratic problem derivaitve to be solved by the optimizer\n",
    "second_gradient_problem = second_derivative_problem_11 # Choose 6th quadratic problem hessian to be solved by the optimizer \n",
    "x_optimal = get_global_minimizer(problem_11, abc_11)\n",
    "# We could choose different problems and problem gradients, but that does not make sense, therefore it is not recommended :).\n",
    "alt_gradient_flag = False # Here one can activate (True), the approximated gradient or deactivate (False) it.\n",
    "alt_stopping_crit = True # Here one can activate (True), the advanced stopping criterion or deactivate (False) it.\n",
    "\n",
    "x_hat = newton_uni(x_start, problem,  gradient_problem, second_gradient_problem, activate_alt_gradient=alt_gradient_flag, activate_new_stop_crit=alt_stopping_crit)\n",
    "final_printout(x_0=x_start, x_optimal=x_optimal, x_appr=x_hat, f=problem, grad=gradient_problem, args={}, tolerance=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
