{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Group 7<br>\n",
    "Participants information in alphabetical order</i>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">Alexander</td>\n",
    "    <td style = \"text-align: left\">Mair</td>\n",
    "    <td style = \"text-align: left\">k11916624</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Andreas</td>\n",
    "    <td style = \"text-align: left\">Oberdammer</td>\n",
    "    <td style = \"text-align: left\">k11908776</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">3</td>\n",
    "    <td style = \"text-align: left\">Dominik</td>\n",
    "    <td style = \"text-align: left\">Zauner</td>\n",
    "    <td style = \"text-align: left\">k11717988</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">4</td>\n",
    "    <td style = \"text-align: left\">Florian</td>\n",
    "    <td style = \"text-align: left\">Rothkegel</td>\n",
    "    <td style = \"text-align: left\">k11908775</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">5</td>\n",
    "    <td style = \"text-align: left\">Martin</td>\n",
    "    <td style = \"text-align: left\">Stockinger</td>\n",
    "    <td style = \"text-align: left\">k01035089</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">6</td>\n",
    "    <td style = \"text-align: left\">Martin</td>\n",
    "    <td style = \"text-align: left\">Zwiffl</td>\n",
    "    <td style = \"text-align: left\">k11910668</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
    "<i>Describe how to install additional packages, if you have some, here</i><br><br>\n",
    "\n",
    "<b>No additional libaries are needed/installed</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
    "<i>Place for additional comments and argumentation</i><br><br>\n",
    "We implemented the stopping criterion based on the hints given in the second phase PDF. Description of the funciton can be found in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for stopping criterium\n",
    "def stopping_criteria(x_start, x_curr, x_prev, x_after_start, fn, gradient_fn, iteration, tolerance=1e-7, max_steps=1e6):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_start: Start point coordinates. Postion on which the algorithm should start\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_curr: Current point coordinates. Postion on which the algorithm is currently\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_prev: Previous point coordinates. Postion on which the algorithm is was in the step before\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_after_start: Fist calculated point coordinates. Postion on which the algorithm was after the first step\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    iteration: Number of the current iteration the algorithm is in. Starts with 0. Is used to check max_steps\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    tolerance: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    max_steps: Number of the current iteration the algorithm is in. Starts with 0. Is used to check max_steps\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - this function checks if the gradient of the current point is below the tolerance\n",
    "    - Additionally it checks if the algo exceeds the allowed max steps\n",
    "    - It is also checked, if the gradient of the current step is smaller than the gradient in the start point\n",
    "      times our tolerance. In our case we noticed by testing, that the algorithm stops a bit too soon, therefore\n",
    "      we introduced a correction factor, which additionally lowers the error bound. Again --> Payoff runtime/acc\n",
    "    - As mentioned in the second phase pdf, we can make the stopping crit relative, by comparing it to the \n",
    "      tolerance multiplied with the difference from the start to the first point. this can be done for f(x) and x.\n",
    "    \"\"\"\n",
    "    correction_factor = 1e-5 # factor to adjust error bound and gain more precicsion (later stopping). Explained above.\n",
    "    \n",
    "    if np.linalg.norm(gradient_fn(x_curr)) <= tolerance:\n",
    "        print(\"*** GRADIENT BELOW TOLERANCE ***\")\n",
    "        return True\n",
    "    \n",
    "    if iteration >= max_steps:\n",
    "        print(\"*** MAX STEPS EXCEEDED ***\")\n",
    "        return True\n",
    "    \n",
    "    if np.linalg.norm(gradient_fn(x_curr)) <= tolerance * np.linalg.norm(gradient_fn(x_start)) * correction_factor:\n",
    "        print(\"*** GRADIENT SMALLER THAN GRADIENT OF FIRST STEP * TOLERANCE ***\")\n",
    "        return True\n",
    "    \n",
    "    if x_prev is not None and x_after_start is not None:\n",
    "        x_diff_first_step = np.linalg.norm(x_after_start - x_start)\n",
    "        y_diff_first_step = np.abs(fn(x_after_start) - fn(x_start))\n",
    "\n",
    "        if np.abs(fn(x_curr) - fn(x_prev)) <= tolerance * y_diff_first_step * correction_factor:\n",
    "            print(\"*** Y DIFFERENCE SMALLER THAN ADJUSTED TOLERANCE ***\")\n",
    "            return True\n",
    "        \n",
    "        if np.linalg.norm(x_curr - x_prev) <= tolerance * x_diff_first_step * correction_factor:\n",
    "            print(\"*** X DIFFERENCE SMALLER THAN ADJUSTED TOLERANCE ***\")\n",
    "            return True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scaling</h1>\n",
    "    \n",
    "Scaling is a major topic when it comes to improving the performance and stability of an optimization algorithm. Generally, it can be viewed as harmonizing the magnitudes of variables. Scaling does not seem to have a pinpoint definition in literature but at the same time seems to be commonly understood. In our examples, we want to take a look at scaling by variable transformation:\n",
    "    \n",
    "<i>\"Scaling by variable transformation converts the variables from units that typically reflect the physical nature of the problem to units that display certain desirable properties during the minimization process.\"</i> - Gill, Murray, Wright (1981): Practical Optimization\n",
    "\n",
    "Also, in some online resources we find an explanation of : <i>\"Scale x so its components are “around 1”\"</i> <a href=\"http://www.fitzgibbon.ie/optimization-parameter-scaling\">LINK</a>\n",
    "\n",
    "Also, it should be noted that some algorithms are sensible to scaledness/unscaledness of a problem. One of those is Steepest Descent/Gradient Descent. Therefore we will take a look at our Gradient Descent implementation regarding scaling.\n",
    "\n",
    "To line out the basic idea more practically, consider the following simple objective functions that should be minimized:\n",
    "\n",
    "<ul> \n",
    "    <li>$f(x,y) = 1000x^2+10y^2$</li>\n",
    "    <li>$f(x,y) = 10^6x^2+10^6y^2$</li> \n",
    "</ul>\n",
    "\n",
    "The first function can be scaled by a diagonal matrix $D=\\begin{bmatrix}\\frac{1}{1000} & 0 \\\\ 0 & \\frac{1}{10} \\end{bmatrix}$\n",
    "\n",
    "The second function can be viwed as scaled by just dividing it straightforward by $10^6$.\n",
    "\n",
    "But just scaling the the inputs does not really affect the inner workings of our algorithms. We make use of the logic shown in our example scaled Gradient Descent below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scaling and our problems</h3>\n",
    "\n",
    "Generally, since the entries of our base matrices A_x for our multivariate Problems range only between 0 and 9, the influences of single variables are somewhat of similar magnitude. Simplifiying this thought by stating that the term $x^TAx$ comes out the polynomials with factors of magnitude 10 - 100, we argue that we can scale down the variables just by $\\frac{1}{10}$ to get closer to the goal of <i>\"scaling x to, so its components are around 1\"</i>. So we do not use scaling diagonal matrix but simply a factor (can be seen as a diagonal matrix with all the same values).\n",
    "\n",
    "In a nutshell, scaling should not be super-crucial for our somewhat nicely scaled problems. As we will see later, we can still save some iterations on our implementation. If we had many more strongly diverging magnitudes of variable impact, it can become a key point for success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def problem_sclaing(xk, gradient_fn, scaling, activate_new_stop_crit, epsilon, fn):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        --------------------------------------------------------------------------------------------------------------\n",
    "        xk: Start point coordinates. Postion on which the algorithm should start\n",
    "        --------------------------------------------------------------------------------------------------------------\n",
    "        fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "        --------------------------------------------------------------------------------------------------------------\n",
    "        gradient_fn: function, which is the gradient of the problem. \n",
    "                     One can parse coordinates to this function to get the gradient.\n",
    "        --------------------------------------------------------------------------------------------------------------\n",
    "        scaling: Value, which defines the strength of the scaling algorithm. > Stronger scaling.\n",
    "        --------------------------------------------------------------------------------------------------------------\n",
    "        epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "                 algorithm --> basic stopping criterion\n",
    "        --------------------------------------------------------------------------------------------------------------\n",
    "        activate_new_stop_crit: Boolean Flag, which de/activates the usage of the additinal stopping criterations.\n",
    "                                If activated, the implemented stopping crit function is called and all features\n",
    "                                get checked.\n",
    "        --------------------------------------------------------------------------------------------------------------\n",
    "        - Scale xk with the scaling factor in every iteration to get yk\n",
    "        - Evaluate gradient/objective on initial unscaled xk\n",
    "        - Get step size\n",
    "        - Then operate on the scaled yk\n",
    "        - The result needs to be rescaled upon returning\n",
    "        \"\"\"\n",
    "        x_start = xk\n",
    "        yk = scaling * xk\n",
    "        prev_x = None\n",
    "        x_step_1 = None\n",
    "        i = 0\n",
    "        while True:\n",
    "            prev_x = xk\n",
    "            xk = yk / scaling\n",
    "            \n",
    "            if i == 1:\n",
    "                x_step_1 = xk\n",
    "            \n",
    "            grad = gradient_fn(xk) * scaling\n",
    "            grad_norm = np.linalg.norm(grad / scaling)\n",
    "\n",
    "            if activate_new_stop_crit and stopping_criteria(x_start, xk, prev_x, x_step_1, fn, gradient_fn, i):\n",
    "                break\n",
    "\n",
    "            elif not activate_new_stop_crit and grad_norm <= epsilon:\n",
    "                break\n",
    "\n",
    "            alpha = backtracking_line_search(xk, fn, gradient_fn, alpha=1, phi=0.9, c=0.0001)\n",
    "            yk = yk - (alpha * grad)\n",
    "            i += 1\n",
    "\n",
    "        print(\"performed \" + str(i+1) + \" iterations.\")\n",
    "\n",
    "        return yk / scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.</i>\n",
    "<br><br>\n",
    "Stability is a really important topic for those type of algorithms, because if the stability is not given, it is not ensured that we even reach our goal, because the algorithm could do basically anything and even crash.<br>\n",
    "Therefore it is really important to take some measures to ensure that the algorithm is stable and works fine.<br><br>\n",
    "\n",
    "Possible Measures:<br>\n",
    "- Avoid floating points problems and round off errors<br>\n",
    "- Scale problems if necessary<br>\n",
    "- Avoid matrix inversion<br>\n",
    "- Use optimal hyperparamters for algorithm<br>\n",
    "- Improve stopping criterion (limit max steps, verify multiple attributes)<br><br>\n",
    "\n",
    "We implemented some of those measure. We tuned our tolerance (epsilon) and the step size to avoid overshooting of the algorithm. We also use float calculation to avoid big round off errors (read more in floating point section).<br>\n",
    "We also use LU decomposition to avoid direct matrix inversion and we also improved our stopping criterion. These measure have a positive impact on the stability of our algorithm. <br>\n",
    "<br>\n",
    "There are for sure more stability improvements, but as we can see on our tests, the algorithm is performing reasonable well and therefore we did not implement more features. <br><br>\n",
    "\n",
    "<b>Below we want to show a described problem, if we would not use default float arrays for our calculation.</b><br>\n",
    "You should be able to see that the numpy result (which is correct result) is equal to the result, if we typecast the int array to an float array. If we just use the int array, we get wrong results and this destroys the stability of the aglo and makes it nearly unuseable.\n",
    "<br><b>Therefore is the useage of float a major contribution for stability </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "MATRIX INVERSION WITH AND WITHOUT FLOAT CALCULATION\n",
      "============================================================================\n",
      "INVERSE MATRIX WITHOUT PRE FLOAT TYPECASTING\n",
      "[[0.5  0.25 0.5 ]\n",
      " [0.   0.5  1.  ]\n",
      " [0.   0.   1.  ]]\n",
      "============================================================================\n",
      "INVERSE MATRIX WITH PRE FLOAT TYPECASTING\n",
      "[[-1.11022302e-16  1.00000000e+00  2.00000000e+00]\n",
      " [-1.00000000e+00  2.00000000e+00  4.00000000e+00]\n",
      " [-1.00000000e+00  2.00000000e+00  5.00000000e+00]]\n",
      "============================================================================\n",
      "INVERSE MATRIX NUMPY CALCULATION (TRUE RESULT)\n",
      "[[-1.11022302e-16  1.00000000e+00  2.00000000e+00]\n",
      " [-1.00000000e+00  2.00000000e+00  4.00000000e+00]\n",
      " [-1.00000000e+00  2.00000000e+00  5.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#your function for stabilising goes here\n",
    "def lu_deco_inverse_not_float(A):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    A: Input matrix, from which the inverse has to be calculated \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - LU Decompostion matrix factorization, creates two trinangular matrices L and U. In this case this is a bit\n",
    "      simpler. We create a permuation matrix and a matrix (a), which is a combination of L and U.\n",
    "    - Based on this matrices we are able to create the inverted version of the given matrix\n",
    "    - Float conversion is missing in this function to show the problems, which happen with no typecasting and int\n",
    "      calculation\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    a = np.copy(A)\n",
    "    p = np.eye(n)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        j = i + np.argmax(np.abs(a[i:, i]))  # maximum in column\n",
    "        if np.abs(a[i, j]) < 1e-8:\n",
    "            print(\"Matrix is singular\")\n",
    "            sys.exit()\n",
    "\n",
    "        if i != j:\n",
    "            a[[i, j]] = a[[j, i]]\n",
    "            p[[i, j]] = p[[j, i]]\n",
    "\n",
    "        for i_row in range(i + 1, n):\n",
    "            a[i_row, i] = a[i_row, i] / a[i, i]\n",
    "        for i_row in range(i + 1, n):\n",
    "            for i_col in range(i + 1, n):\n",
    "                a[i_row, i_col] = a[i_row, i_col] - np.outer(a[i_row, i], a[i, i_col])\n",
    "\n",
    "    a_inv = np.copy(np.dot(p, np.eye(n)))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            a_inv[i] -= np.dot(a[i, j], a_inv[j])  # Forward substitution\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        for j in range(i + 1, n):\n",
    "            a_inv[i] = a_inv[i] - np.dot(a[i, j], a_inv[j])  # Backward substitution\n",
    "        a_inv[i] = a_inv[i] / a[i, i]\n",
    "\n",
    "    return a_inv\n",
    "\n",
    "def lu_deco_inverse_float(A):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    A: Input matrix, from which the inverse has to be calculated \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - LU Decompostion matrix factorization, creates two trinangular matrices L and U. In this case this is a bit\n",
    "      simpler. We create a permuation matrix and a matrix (a), which is a combination of L and U.\n",
    "    - Based on this matrices we are able to create the inverted version of the given matrix\n",
    "    - Float conversion is done here to get the correct results (no round of error/auto typecasting problem)\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    a = np.copy(A.astype(float))\n",
    "    p = np.eye(n)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        j = i + np.argmax(np.abs(a[i:, i]))  # maximum in column\n",
    "        if np.abs(a[i, j]) < 1e-8:\n",
    "            print(\"Matrix is singular\")\n",
    "            sys.exit()\n",
    "\n",
    "        if i != j:\n",
    "            a[[i, j]] = a[[j, i]]\n",
    "            p[[i, j]] = p[[j, i]]\n",
    "\n",
    "        for i_row in range(i + 1, n):\n",
    "            a[i_row, i] = a[i_row, i] / a[i, i]\n",
    "        for i_row in range(i + 1, n):\n",
    "            for i_col in range(i + 1, n):\n",
    "                a[i_row, i_col] = a[i_row, i_col] - np.outer(a[i_row, i], a[i, i_col])\n",
    "\n",
    "    a_inv = np.copy(np.dot(p, np.eye(n)))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            a_inv[i] -= np.dot(a[i, j], a_inv[j])  # Forward substitution\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        for j in range(i + 1, n):\n",
    "            a_inv[i] = a_inv[i] - np.dot(a[i, j], a_inv[j])  # Backward substitution\n",
    "        a_inv[i] = a_inv[i] / a[i, i]\n",
    "\n",
    "    return a_inv\n",
    "\n",
    "test_mat = np.array([[2,-1,0], [1,2,-2], [0,-1,1]]) # matrix to test scenario on. Can be chosen abitrarly\n",
    "\n",
    "print(\"============================================================================\")\n",
    "print(\"MATRIX INVERSION WITH AND WITHOUT FLOAT CALCULATION\")\n",
    "print(\"============================================================================\")\n",
    "print(\"INVERSE MATRIX WITHOUT PRE FLOAT TYPECASTING\")\n",
    "print(lu_deco_inverse_not_float(test_mat))\n",
    "print(\"============================================================================\")\n",
    "print(\"INVERSE MATRIX WITH PRE FLOAT TYPECASTING\")\n",
    "print(lu_deco_inverse_float(test_mat))\n",
    "print(\"============================================================================\")\n",
    "print(\"INVERSE MATRIX NUMPY CALCULATION (TRUE RESULT)\")\n",
    "print(np.linalg.inv(test_mat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
    "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.</i>\n",
    "<br><br>\n",
    "In our algorithm and calculations we only use floating point numbers. There is always the risk of roundoff errors and therefore loss in accuracy, which leads to increased error rates.<br>\n",
    "We are fully aware of this problem and therefore suggest the following options:<br>\n",
    "- Do every calculation in float, in order to avoid typecasting and high round of errors<br>\n",
    "- Increase precision of float (assign more memory (bits)) to the variables and reduce error (increased memory usage + computaton time)<br>\n",
    "- We could also use a special python libary for high precision calculation. Same as before, this increases memory usage and computation time.<br><br>\n",
    "\n",
    "As we can see, there are some measure we can take, in order to avoid round off errors. The most important measure is the first one. If we calculate everything in float, the round off error is small enough <br>\n",
    "and as we can see, based on our tests below, we do not have any problems, therefore this measure seems to be enough. If we really have the need for super high precision calculation we have to either increase the<br>\n",
    "memory size of float or use an external libary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
    "<i>Place for additional comments and argumentation</i><br><br>\n",
    "    \n",
    "The algorithm of choice for inverting matrices is based on the approach from the book <b> Numerical Optimization</b> written by <b><i> Jorge Nocedal, Stephen J. Wright</i></b> and the calculator of TU Graz (http://lampx.tugraz.at/~hadley/num/ch2/2.3.php).<br>\n",
    "We do not directly decompose the matrix into L and U, because this not really needed in our case, since we reuse the LU matrix and do not use the two parts separate. <i>a_inv</i> is basically the the LU matrix, which is than used to create inverse.\n",
    "<br><br><b>IMPORTANT:</b> This is also mentioned in the floating point/error section. If the matrix is parsed with int values, python has to do typecasting, because of the divison, which is done in this function. As far as our testing goes, typecasting leads<br>\n",
    "to an major error in calculation and leads to wrong inverse matrices. This is not only the case for this function, but also in some other cases we encountered this problem. For more details check the floating point/stability section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for invertion goes here\n",
    "def lu_deco_inverse(A):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    iteration: Number of the current iteration the algorithm is in. Starts with 0. Is used to check max_steps\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    tolerance: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    max_steps: Number of the current iteration the algorithm is in. Starts with 0. Is used to check max_steps\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - this function checks if the gradient of the current point is below the tolerance\n",
    "    - Additionally it checks if the algo exceeds the allowed max steps\n",
    "    - It is also checked, if the gradient of the current step is smaller than the gradient in the start point\n",
    "      times our tolerance. In our case we noticed by testing, that the algorithm stops a bit too soon, therefore\n",
    "      we introduced a correction factor, which additionally lowers the error bound. Again --> Payoff runtime/acc\n",
    "    - As mentioned in the second phase pdf, we can make the stopping crit relative, by comparing it to the \n",
    "      tolerance multiplied with the difference from the start to the first point. this can be done for f(x) and x.\n",
    "\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - LU Decompostion matrix factorization, creates two trinangular matrices L and U. In this case this is a bit\n",
    "      simpler. We create a permuation matrix and a matrix (a), which is a combination of L and U.\n",
    "    - Based on this matrices we are able to create the inverted version of the given matrix\n",
    "    - Float conversion is done here to get the correct results (no round of error/auto typecasting problem)\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    a = np.copy(A.astype(float))\n",
    "    p = np.eye(n)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        j = i + np.argmax(np.abs(a[i:, i]))  # maximum in column\n",
    "        if np.abs(a[i, j]) < 1e-8:\n",
    "            print(\"Matrix is singular\")\n",
    "            sys.exit()\n",
    "\n",
    "        if i != j:\n",
    "            a[[i, j]] = a[[j, i]]\n",
    "            p[[i, j]] = p[[j, i]]\n",
    "\n",
    "        for i_row in range(i + 1, n):\n",
    "            a[i_row, i] = a[i_row, i] / a[i, i]\n",
    "        for i_row in range(i + 1, n):\n",
    "            for i_col in range(i + 1, n):\n",
    "                a[i_row, i_col] = a[i_row, i_col] - np.outer(a[i_row, i], a[i, i_col])\n",
    "\n",
    "    a_inv = np.copy(np.dot(p, np.eye(n)))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            a_inv[i] -= np.dot(a[i, j], a_inv[j])  # Forward substitution\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        for j in range(i + 1, n):\n",
    "            a_inv[i] = a_inv[i] - np.dot(a[i, j], a_inv[j])  # Backward substitution\n",
    "        a_inv[i] = a_inv[i] / a[i, i]\n",
    "\n",
    "    return a_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
    "<i>Place for additional comments and argumentation</i>\n",
    "<br><br>  \n",
    "As suggested, we now approximate derivative and hessians. We did this by using the trivial formulas of section 8.1 (central- difference). <br>\n",
    "4 wrapper functions were created, because the different shapes, so we can use the aproximated derivatives, gradients, second_derivatives and hessians just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for gradient approximation goes here\n",
    "def differential_quotient_helper(fn, x, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x: point, which should be taken into account. Mostly used for the shape to recreate correct der/grad shape\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Algorithm, which approximates numericaly the gradient of a given function f\n",
    "    \"\"\"\n",
    "    res=[]\n",
    "    x = x.astype(float)\n",
    "    x=x.reshape(x.shape[1] if len(x.shape) > 1 and x.shape[1] > x.shape[0] else x.shape[0])\n",
    "    for idx in range(len(x)):\n",
    "        direction = np.zeros(len(x))\n",
    "        direction[idx] = 1\n",
    "        part_derivative = (fn(x+epsilon*direction)-fn(x-epsilon*direction))/(2*epsilon)\n",
    "        res.append(part_derivative)\n",
    "    return np.array(res)\n",
    "\n",
    "def grad_wrapper(fn,  epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Wrapper, which returns the useable gradient function of a given problem function fn\n",
    "    \"\"\"\n",
    "    return lambda x: differential_quotient_helper(fn, x,  epsilon)\n",
    "\n",
    "def hessian_wrapper(fn, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Wrapper, which returns the useable hessian function of a given problem function fn\n",
    "    \"\"\"\n",
    "    return lambda x: grad_wrapper(grad_wrapper(fn, epsilon), epsilon)(x).reshape(len(x),len(x))\n",
    "\n",
    "def derivative_wrapper(fn, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Wrapper, which returns the useable derivaitve function of a given problem function fn\n",
    "    \"\"\"\n",
    "    return lambda x: (fn(x+epsilon)-fn(x-epsilon))/(2*epsilon)\n",
    "\n",
    "def second_derivative_wrapper(fn, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - Wrapper, which returns the useable second derivative function of a given problem function fn\n",
    "    \"\"\"\n",
    "    return lambda x: derivative_wrapper(derivative_wrapper(fn, epsilon), epsilon)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def backtracking_line_search(x, fn, gradient_fn, alpha, phi, c):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x: Start point coordinates. Postion on which the algorithm should start\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    alpha - phi - c: Line search specific hyperparameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - backtracking line search used for steepest descent\n",
    "    \"\"\"\n",
    "    pk = - gradient_fn(x)\n",
    "    left = fn(x + alpha * pk)\n",
    "    right = fn(x) + c * alpha * (gradient_fn(x).reshape(-1) @ pk.reshape(-1))\n",
    "    while left > right:\n",
    "        alpha = alpha * phi\n",
    "        left = fn(x + alpha * pk)\n",
    "        right = fn(x) + c * alpha * (gradient_fn(x).reshape(-1) @ pk.reshape(-1))\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "# Our Main SD algorithm for multivariate functions\n",
    "def steepest_descent_multi(x_start, fn, gradient_fn, epsilon=0.8e-5,\n",
    "         activate_new_stop_crit=False,\n",
    "         activate_pre_scaling=False,\n",
    "         activate_alt_gradient=False,\n",
    "         activate_new_matrix_inversion=False,\n",
    "         scaling=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_start: Start point coordinates. Postion on which the algorithm should start\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_new_stop_crit: Boolean Flag, which de/activates the usage of the additinal stopping criterations.\n",
    "                            If activated, the implemented stopping crit function is called and all features\n",
    "                            get checked.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_pre_scaling: Boolean Flag, to activate pre scaling of the problem. In this case, we have not \n",
    "                          implemented pre scaling.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_alt_gradient: Boolean Flag, to activate alternative gradient calculation. This calls the gradient\n",
    "                           wrapper and approximates the gradient function.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_new_matrix_inversion: Boolean Flag, to activate alternative matrix inversion. This calls the LU\n",
    "                                   decomposition function, but is not needed in this algo since nothing needs to\n",
    "                                   be inversed.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - steepest descent method for quadractic multi variate examples\n",
    "    \"\"\"\n",
    "    if activate_alt_gradient:\n",
    "        gradient_fn = grad_wrapper(fn)\n",
    "    \n",
    "    if not activate_pre_scaling:\n",
    "        i = 0\n",
    "        xk = x_start\n",
    "        prev_x = None\n",
    "        x_step_1 = None \n",
    "        while True:  \n",
    "            grad = gradient_fn(xk)\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "            if not activate_new_stop_crit and grad_norm <= epsilon:\n",
    "                break\n",
    "\n",
    "            elif activate_new_stop_crit and stopping_criteria(x_start, xk, prev_x, x_step_1, fn, gradient_fn, i):\n",
    "                break\n",
    "\n",
    "            alpha = backtracking_line_search(xk, fn, gradient_fn, alpha=1, phi=0.9, c=0.0001)\n",
    "            prev_x = xk\n",
    "            xk = xk - (alpha * grad)\n",
    "            \n",
    "            if i == 0:\n",
    "                x_step_1 = xk\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "        print(\"performed \" + str(i+1) + \" iterations.\")\n",
    "\n",
    "        return xk\n",
    "\n",
    "    elif activate_pre_scaling:\n",
    "        if scaling is None:\n",
    "            raise ValueError('You have to choose a value for *scaling*')\n",
    "        return  problem_sclaing(x_start, gradient_fn, scaling, activate_new_stop_crit, epsilon, fn)\n",
    "    \n",
    "\n",
    "\n",
    "# Our Main SD algorithm for univariate functions\n",
    "def steepest_descent_uni(x_start, fn, derivative, lr=1e-7, epsilon=1e-5,\n",
    "         activate_new_stop_crit=False,\n",
    "         activate_pre_scaling=False,\n",
    "         activate_alt_gradient=False,\n",
    "         activate_new_matrix_inversion=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_start: Start point coordinates. Postion on which the algorithm should start\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    fn: function, which is the problem. One can parse coordinates to this function to get y-axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    gradient_fn: function, which is the gradient of the problem. \n",
    "                 One can parse coordinates to this function to get the gradient.  \n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    lr: learning rate, which defines the size of the steps the algorithm can make per iteration. \n",
    "        Definese the aggresivity of the algorithm, can increase/decrease overshoot.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    epsilon: tolerance of the algorithm. If this values is reached and our values are below it, we can stop the\n",
    "             algorithm --> basic stopping criterion\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_new_stop_crit: Boolean Flag, which de/activates the usage of the additinal stopping criterations.\n",
    "                            If activated, the implemented stopping crit function is called and all features\n",
    "                            get checked.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_pre_scaling: Boolean Flag, to activate pre scaling of the problem. In this case, we have not \n",
    "                          implemented pre scaling.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_alt_gradient: Boolean Flag, to activate alternative gradient calculation. This calls the gradient\n",
    "                           wrapper and approximates the gradient function.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    activate_new_matrix_inversion: Boolean Flag, to activate alternative matrix inversion. This calls the LU\n",
    "                                   decomposition function, but is not needed in this algo since nothing needs to\n",
    "                                   be inversed.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - steepest descent method for above quadratic uni variate examples\n",
    "    \"\"\"\n",
    "    if activate_alt_gradient:\n",
    "        derivative = derivative_wrapper(fn)\n",
    "    \n",
    "    xk = x_start\n",
    "    prev_x = None\n",
    "    x_step_1 = None\n",
    "    i = 0\n",
    "    while True:\n",
    "        der = derivative(xk)\n",
    "        \n",
    "        if i == 1:\n",
    "            x_step_1 = xk\n",
    "\n",
    "        if activate_new_stop_crit and stopping_criteria(x_start, xk, prev_x, x_step_1, fn, derivative, i):\n",
    "            break\n",
    "\n",
    "        elif not activate_new_stop_crit and abs(der) <= epsilon:\n",
    "            break\n",
    "        \n",
    "        prev_x = xk\n",
    "        xk=xk-(lr * der)\n",
    "        i += 1\n",
    "    print(\"performed \" + str(i+1) + \" iterations.\")\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def get_random_A_b_minimizer(seed):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    seed: Seed is an int value, which is used to create reproducable random values.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - 10x10 random matrix is produced by this function, which is the base of the problem\n",
    "    - 10x1 random vector is produced, which is also the minimizer of the function and the goal of the algo\n",
    "    - Function uses thoes variables to create a problem function, which can be solved by the algo\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    A=np.random.randint(0, 2, (10, 10))\n",
    "    A=A@A.T\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    minimizer=np.random.randint(0, 10, (10, 1))\n",
    "\n",
    "    b=A@minimizer\n",
    "\n",
    "    return A, b, minimizer\n",
    "\n",
    "# Created problem instances, which are used in the functions below, to provide a useable set of prolems for the algo, A and b correspond to function depended values and Minimizer is the solution to the problem\n",
    "A_1, b_1, minimizer_1 = get_random_A_b_minimizer(0)\n",
    "A_2, b_2, minimizer_2 = get_random_A_b_minimizer(2)\n",
    "A_3, b_3, minimizer_3 = get_random_A_b_minimizer(3)\n",
    "A_4, b_4, minimizer_4 = get_random_A_b_minimizer(12)\n",
    "A_5, b_5, minimizer_5 = get_random_A_b_minimizer(15)\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# problem_x_q(x) --> is the problem function f, which is used by algorithm.\n",
    "# grad_problem_X_q(x) --> as the name implies, this function is the gradient of the problem function f (problem_X_q)\n",
    "# shessian_problem_X_q(x) --> as the name implies, this function is the hessian of the problem function f (problem_X)\n",
    "# in all cases you can parse x coordinates to the functions in order to receive the corresponding y axis values.\n",
    "#############################################################################################################################################################\n",
    "# Problem 1\n",
    "\n",
    "def problem_1_q(x):\n",
    "    return (x.T@A_1@x)/2-b_1.T@x\n",
    "\n",
    "def grad_problem_1_q(x):\n",
    "    return np.dot(A_1,x)-b_1\n",
    "\n",
    "def hessian_problem_1_q(x):\n",
    "    return A_1\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 2\n",
    "\n",
    "def problem_2_q(x):\n",
    "    return (x.T@A_2@x)/2-b_2.T@x\n",
    "\n",
    "def grad_problem_2_q(x):\n",
    "    return A_2@x-b_2\n",
    "\n",
    "def hessian_problem_2_q(x):\n",
    "    return A_2\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 3\n",
    "\n",
    "def problem_3_q(x):\n",
    "    return (x.T@A_3@x)/2-b_3.T@x\n",
    "\n",
    "def grad_problem_3_q(x):\n",
    "    return A_3@x-b_3\n",
    "\n",
    "def hessian_problem_3_q(x):\n",
    "    return A_3\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 4\n",
    "\n",
    "def problem_4_q(x):\n",
    "    return (x.T@A_4@x)/2-b_4.T@x\n",
    "\n",
    "def grad_problem_4_q(x):\n",
    "    return A_4@x-b_4\n",
    "\n",
    "def hessian_problem_4_q(x):\n",
    "    return A_4\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 5\n",
    "\n",
    "def problem_5_q(x):\n",
    "    return (x.T@A_5@x)/2-b_5.T@x\n",
    "\n",
    "def grad_problem_5_q(x):\n",
    "    return A_5@x-b_5\n",
    "\n",
    "def hessian_problem_5_q(x):\n",
    "    return A_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><b>Note:</b> After every test print out the resulsts. \n",
    "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
    "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_0: numpy 1D array, corresponds to initial point\n",
    "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
    "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
    "                                      **args\n",
    "       Function f returns a scalar output.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
    "                                         function f,\n",
    "                                         args (which are submitted, because you might need\n",
    "                                              to call f(x,**args) inside your gradient function implementation). \n",
    "          Function grad approximates gradient at given point and returns a 1d np array.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    args: dictionary, additional (except of x) arguments to function f\n",
    "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Initial x is :\\t\\t{x_0}')\n",
    "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
    "    print(f'Approximated x is :\\t{x_appr}')\n",
    "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
    "    f_opt = f(x_optimal)\n",
    "    f_appr = f(x_appr)\n",
    "    print(f'Function value in optimal point:\\t{f_opt}')\n",
    "    print(f'Function value in approximated point:   {f_appr}')\n",
    "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
    "    print(f'Gradient approximation in optimal point is:\\n{grad(x_optimal)}\\n')\n",
    "    grad_appr = grad(x_appr)\n",
    "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
    "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "DEFAULT TESTS WITHOUT ANY ADDITIONAL FEATURES\n",
      "============================================================================\n",
      "**** PROBLEM 1 ****\n",
      "performed 27426 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99978826e+00]\n",
      " [-1.50251160e-04]\n",
      " [ 2.99993373e+00]\n",
      " [ 3.00033758e+00]\n",
      " [ 7.00007641e+00]\n",
      " [ 9.00028584e+00]\n",
      " [ 2.99966083e+00]\n",
      " [ 5.00008874e+00]\n",
      " [ 1.99979429e+00]\n",
      " [ 4.00005632e+00]]\n",
      "Is close verificaion: \t[[False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.39131356e-06]\n",
      " [-2.35047878e-07]\n",
      " [ 1.60550849e-06]\n",
      " [ 3.51312744e-06]\n",
      " [ 2.50869056e-06]\n",
      " [ 3.16264908e-06]\n",
      " [-9.97649380e-07]\n",
      " [ 2.00431822e-06]\n",
      " [ 1.17500591e-06]\n",
      " [ 1.85288403e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 2 ****\n",
      "performed 3981 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [8]\n",
      " [6]\n",
      " [2]\n",
      " [8]\n",
      " [7]\n",
      " [2]\n",
      " [1]\n",
      " [5]\n",
      " [4]]\n",
      "Approximated x is :\t[[8.00003225]\n",
      " [7.99996271]\n",
      " [5.99997152]\n",
      " [2.00002962]\n",
      " [8.00004182]\n",
      " [6.99999909]\n",
      " [1.99999432]\n",
      " [0.99997449]\n",
      " [5.00004199]\n",
      " [4.00000118]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3230.]]\n",
      "Function value in approximated point:   [[-3230.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.38759219e-07]\n",
      " [-2.44076983e-06]\n",
      " [-1.58346673e-06]\n",
      " [ 7.07352399e-08]\n",
      " [ 4.57151685e-07]\n",
      " [-1.00930276e-06]\n",
      " [-1.24244877e-06]\n",
      " [-1.75762858e-06]\n",
      " [ 1.01226241e-06]\n",
      " [-4.71727191e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 3 ****\n",
      "performed 5729 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [9]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [5]\n",
      " [3]\n",
      " [9]\n",
      " [9]]\n",
      "Approximated x is :\t[[8.00002036e+00]\n",
      " [8.99997567e+00]\n",
      " [3.00004372e+00]\n",
      " [8.00003890e+00]\n",
      " [8.00003752e+00]\n",
      " [4.31297739e-06]\n",
      " [4.99997238e+00]\n",
      " [2.99996124e+00]\n",
      " [8.99996624e+00]\n",
      " [9.00000754e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-5567.5]]\n",
      "Function value in approximated point:   [[-5567.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-1.86749133e-06]\n",
      " [-2.82678650e-06]\n",
      " [-1.76769163e-07]\n",
      " [-1.22997432e-06]\n",
      " [-1.67115459e-06]\n",
      " [-1.91048997e-06]\n",
      " [-2.44195772e-06]\n",
      " [-4.11518894e-06]\n",
      " [-3.61519471e-06]\n",
      " [-1.73034496e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 4 ****\n",
      "performed 16381 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [5]]\n",
      "Approximated x is :\t[[5.99985669e+00]\n",
      " [1.00001736e+00]\n",
      " [2.00011427e+00]\n",
      " [3.00011902e+00]\n",
      " [3.00015573e+00]\n",
      " [6.53996060e-05]\n",
      " [5.99998906e+00]\n",
      " [9.99791714e-01]\n",
      " [4.00003313e+00]\n",
      " [4.99996272e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-1475.5]]\n",
      "Function value in approximated point:   [[-1475.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[1.55761299e-06]\n",
      " [2.51626660e-06]\n",
      " [2.76662870e-06]\n",
      " [2.91988897e-06]\n",
      " [3.13263884e-06]\n",
      " [3.29789228e-06]\n",
      " [2.24516879e-06]\n",
      " [7.09393660e-07]\n",
      " [1.68949711e-06]\n",
      " [2.22233258e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 5 ****\n",
      "performed 1519 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [5]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [5]\n",
      " [6]\n",
      " [1]\n",
      " [7]]\n",
      "Approximated x is :\t[[8.00000260e+00]\n",
      " [5.00000711e+00]\n",
      " [5.00000184e+00]\n",
      " [7.00000311e+00]\n",
      " [7.11023125e-06]\n",
      " [6.99999603e+00]\n",
      " [4.99999404e+00]\n",
      " [5.99999394e+00]\n",
      " [9.99986578e-01]\n",
      " [6.99999626e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4121.5]]\n",
      "Function value in approximated point:   [[-4121.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-1.67317921e-06]\n",
      " [-1.83239632e-06]\n",
      " [-2.03150177e-06]\n",
      " [-1.94784278e-06]\n",
      " [-9.98634221e-07]\n",
      " [-1.66549930e-06]\n",
      " [-2.59637605e-06]\n",
      " [-2.83575054e-06]\n",
      " [-2.63284366e-06]\n",
      " [-2.34196676e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "print(\"============================================================================\")\n",
    "print(\"DEFAULT TESTS WITHOUT ANY ADDITIONAL FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "print(\"**** PROBLEM 1 ****\")\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 2 ****\")\n",
    "x_hat=steepest_descent_multi(init, problem_2_q, grad_problem_2_q)\n",
    "final_printout(init, minimizer_2, x_hat, problem_2_q, grad_problem_2_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 3 ****\")\n",
    "x_hat=steepest_descent_multi(init, problem_3_q, grad_problem_3_q)\n",
    "final_printout(init, minimizer_3, x_hat, problem_3_q, grad_problem_3_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 4 ****\")\n",
    "x_hat=steepest_descent_multi(init, problem_4_q, grad_problem_4_q)\n",
    "final_printout(init, minimizer_4, x_hat, problem_4_q, grad_problem_4_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 5 ****\")\n",
    "x_hat=steepest_descent_multi(init, problem_5_q, grad_problem_5_q)\n",
    "final_printout(init, minimizer_5, x_hat, problem_5_q, grad_problem_5_q, {}, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED SCALING FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 27160 iterations. Final norm of gradient: 7.93894662732799e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99977126e+00]\n",
      " [-1.62317696e-04]\n",
      " [ 2.99992842e+00]\n",
      " [ 3.00036471e+00]\n",
      " [ 7.00008255e+00]\n",
      " [ 9.00030881e+00]\n",
      " [ 2.99963359e+00]\n",
      " [ 5.00009588e+00]\n",
      " [ 1.99977777e+00]\n",
      " [ 4.00006086e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.94171002e-06]\n",
      " [-1.57909788e-07]\n",
      " [ 2.07779323e-06]\n",
      " [ 4.10522560e-06]\n",
      " [ 3.08208456e-06]\n",
      " [ 3.71219630e-06]\n",
      " [-9.43288001e-07]\n",
      " [ 2.43710173e-06]\n",
      " [ 1.66442550e-06]\n",
      " [ 2.27648465e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 27911 iterations. Final norm of gradient: 7.275422002781548e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99981597e+00]\n",
      " [-1.30517264e-04]\n",
      " [ 2.99994234e+00]\n",
      " [ 3.00029307e+00]\n",
      " [ 7.00006623e+00]\n",
      " [ 9.00024814e+00]\n",
      " [ 2.99970541e+00]\n",
      " [ 5.00007698e+00]\n",
      " [ 1.99982122e+00]\n",
      " [ 4.00004882e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-3.91594045e-06]\n",
      " [-1.32583614e-06]\n",
      " [-2.61640827e-06]\n",
      " [-5.69316910e-07]\n",
      " [-2.16567364e-06]\n",
      " [-7.05489413e-07]\n",
      " [-2.43765791e-06]\n",
      " [-1.43436026e-06]\n",
      " [-3.59443297e-06]\n",
      " [-1.60070260e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "VERSUS NO SCALING FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 27426 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99978826e+00]\n",
      " [-1.50251160e-04]\n",
      " [ 2.99993373e+00]\n",
      " [ 3.00033758e+00]\n",
      " [ 7.00007641e+00]\n",
      " [ 9.00028584e+00]\n",
      " [ 2.99966083e+00]\n",
      " [ 5.00008874e+00]\n",
      " [ 1.99979429e+00]\n",
      " [ 4.00005632e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.39131356e-06]\n",
      " [-2.35047878e-07]\n",
      " [ 1.60550849e-06]\n",
      " [ 3.51312744e-06]\n",
      " [ 2.50869056e-06]\n",
      " [ 3.16264908e-06]\n",
      " [-9.97649380e-07]\n",
      " [ 2.00431822e-06]\n",
      " [ 1.17500591e-06]\n",
      " [ 1.85288403e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED SCALING FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q, scaling = 0.1, activate_pre_scaling=True)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q, scaling = 1000, activate_pre_scaling=True)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NO SCALING FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q,  activate_pre_scaling=False)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED ADVANCED STOPPING CRITERION FOR PROBLEM 1\n",
      "============================================================================\n",
      "*** Y DIFFERENCE SMALLER THAN ADJUSTED TOLERANCE ***\n",
      "performed 18962 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99753514e+00]\n",
      " [-1.74993767e-03]\n",
      " [ 2.99922937e+00]\n",
      " [ 3.00393383e+00]\n",
      " [ 7.00089158e+00]\n",
      " [ 9.00333104e+00]\n",
      " [ 2.99604949e+00]\n",
      " [ 5.00103488e+00]\n",
      " [ 1.99760518e+00]\n",
      " [ 4.00065724e+00]]\n",
      "Is close verificaion: \t[[False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.49999984]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[7.97809674e-05]\n",
      " [1.11781764e-05]\n",
      " [6.84615282e-05]\n",
      " [8.58403814e-05]\n",
      " [8.31222080e-05]\n",
      " [7.96707758e-05]\n",
      " [7.87152015e-06]\n",
      " [6.27393827e-05]\n",
      " [7.09418132e-05]\n",
      " [6.14073766e-05]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL STOPPING CRITERION FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 27426 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99978826e+00]\n",
      " [-1.50251160e-04]\n",
      " [ 2.99993373e+00]\n",
      " [ 3.00033758e+00]\n",
      " [ 7.00007641e+00]\n",
      " [ 9.00028584e+00]\n",
      " [ 2.99966083e+00]\n",
      " [ 5.00008874e+00]\n",
      " [ 1.99979429e+00]\n",
      " [ 4.00005632e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.39131356e-06]\n",
      " [-2.35047878e-07]\n",
      " [ 1.60550849e-06]\n",
      " [ 3.51312744e-06]\n",
      " [ 2.50869056e-06]\n",
      " [ 3.16264908e-06]\n",
      " [-9.97649380e-07]\n",
      " [ 2.00431822e-06]\n",
      " [ 1.17500591e-06]\n",
      " [ 1.85288403e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED ADVANCED STOPPING CRITERION FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q, activate_new_stop_crit=True)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL STOPPING CRITERION FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q, activate_new_stop_crit=False)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED APPROXIMATED GRADIENT FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 27711 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99980495e+00]\n",
      " [-1.38336052e-04]\n",
      " [ 2.99993888e+00]\n",
      " [ 3.00031063e+00]\n",
      " [ 7.00007020e+00]\n",
      " [ 9.00026301e+00]\n",
      " [ 2.99968776e+00]\n",
      " [ 5.00008159e+00]\n",
      " [ 1.99981051e+00]\n",
      " [ 4.00005175e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-4.17875214e-06]\n",
      " [-1.41181957e-06]\n",
      " [-2.79459368e-06]\n",
      " [-6.23183155e-07]\n",
      " [-2.31845624e-06]\n",
      " [-7.66840486e-07]\n",
      " [-2.59244639e-06]\n",
      " [-1.53739595e-06]\n",
      " [-3.83395511e-06]\n",
      " [-1.71329233e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL GRADIENT FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 27426 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99978826e+00]\n",
      " [-1.50251160e-04]\n",
      " [ 2.99993373e+00]\n",
      " [ 3.00033758e+00]\n",
      " [ 7.00007641e+00]\n",
      " [ 9.00028584e+00]\n",
      " [ 2.99966083e+00]\n",
      " [ 5.00008874e+00]\n",
      " [ 1.99979429e+00]\n",
      " [ 4.00005632e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.39131356e-06]\n",
      " [-2.35047878e-07]\n",
      " [ 1.60550849e-06]\n",
      " [ 3.51312744e-06]\n",
      " [ 2.50869056e-06]\n",
      " [ 3.16264908e-06]\n",
      " [-9.97649380e-07]\n",
      " [ 2.00431822e-06]\n",
      " [ 1.17500591e-06]\n",
      " [ 1.85288403e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED APPROXIMATED GRADIENT FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q, activate_alt_gradient=True)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL GRADIENT FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q, activate_alt_gradient=False)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED APPROXIMATED GRADIENT AND SCALING FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 27538 iterations. Final norm of gradient: 7.766602168140914e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99979506e+00]\n",
      " [-1.45437433e-04]\n",
      " [ 2.99993587e+00]\n",
      " [ 3.00032679e+00]\n",
      " [ 7.00007398e+00]\n",
      " [ 9.00027670e+00]\n",
      " [ 2.99967170e+00]\n",
      " [ 5.00008591e+00]\n",
      " [ 1.99980089e+00]\n",
      " [ 4.00005453e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 2.06790151e-06]\n",
      " [-7.05327352e-08]\n",
      " [ 2.11876250e-06]\n",
      " [ 3.90935895e-06]\n",
      " [ 3.03987190e-06]\n",
      " [ 3.54683669e-06]\n",
      " [-7.44929309e-07]\n",
      " [ 2.38803963e-06]\n",
      " [ 1.78627889e-06]\n",
      " [ 2.24391206e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL GRADIENT FOR PROBLEM 1\n",
      "============================================================================\n",
      "performed 27426 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]]\n",
      "Approximated x is :\t[[ 4.99978826e+00]\n",
      " [-1.50251160e-04]\n",
      " [ 2.99993373e+00]\n",
      " [ 3.00033758e+00]\n",
      " [ 7.00007641e+00]\n",
      " [ 9.00028584e+00]\n",
      " [ 2.99966083e+00]\n",
      " [ 5.00008874e+00]\n",
      " [ 1.99979429e+00]\n",
      " [ 4.00005632e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3239.5]]\n",
      "Function value in approximated point:   [[-3239.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.39131356e-06]\n",
      " [-2.35047878e-07]\n",
      " [ 1.60550849e-06]\n",
      " [ 3.51312744e-06]\n",
      " [ 2.50869056e-06]\n",
      " [ 3.16264908e-06]\n",
      " [-9.97649380e-07]\n",
      " [ 2.00431822e-06]\n",
      " [ 1.17500591e-06]\n",
      " [ 1.85288403e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED APPROXIMATED GRADIENT AND SCALING FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]])\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q, activate_alt_gradient=True, scaling = 1000, activate_pre_scaling=True)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL GRADIENT FOR PROBLEM 1\")\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, problem_1_q, grad_problem_1_q, activate_alt_gradient=False)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS:\n",
    "\n",
    "#### Stopping Criterion:\n",
    "\n",
    "Our Steepest Algorithm is able to find all global minimizers, which is already really good. In some cases the algorithm is not able to achieve an error rate below 0.001, but this happens not often. <br>We could easily improve this by lowering the epsilon (tolerance), but this is also increases the runtime of the algorithm, therefore we decided to take the middle route and accept some \"higher\" errors. The errors are still below the min requirement of 0.01/0.001. <br><br>\n",
    "Additionally we tested our <b>advanced</b> stopping criterion, which yields major imporvements for the runtime. We were able to reduce the number of iterations by about 10k.<br>\n",
    "This is really helpful, if we want to increase the speed of our algorithm. Everything has its price, in this case we loose some accuracy and the error increases.<br> We still have a reasonable low error, but one has to be aware of that.\n",
    "\n",
    "#### Approximated Gradient:\n",
    "\n",
    "As we can see above, the approxmation algorithm works perfectly. There is only a small difference between the results. If the difference would be larger, <br>\n",
    "we would notice that the algorithm converges to infinity and would be able to find the minimizer. Therefore it is really important to find the right tolerance.<br>\n",
    "Again as already mentioned in the stability section, we have to find the middle way, between low error and fast execution.\n",
    "\n",
    "#### Scaling:\n",
    "\n",
    "In our initial version we used a stable learning rate á la NN gradient descent, but changed it to a simple backtracking line search, which improved the runtime by a big margin.<br>\n",
    "It is hard to tell whether scaling improved stability or something like that, but it apparently cost us a few iterations on our example if we scale down (what we should do according to literature).<br>\n",
    "This could have something to do with the chosen parameters in backtracking line search. It is overall hard to tell if there are improvements. The reason for that are probably our problems. <br>\n",
    "Our problems are not really problematic, in regards to scaling and therefore we do not experience big differences, but it could help with difficult examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "# Stationary points of each problem. Number at the end of abc_X is the problem number. abc indicates the name of the of the 3 stationary points a, b and c.\n",
    "abc_6 = np.array([1, 20, 300])\n",
    "abc_7 = np.array([4, 50, 600])\n",
    "abc_8 = np.array([7, 80, 900])\n",
    "abc_9 = np.array([1, 100, 200])\n",
    "abc_10 = np.array([2, 120, 200])\n",
    "\n",
    "\n",
    "# Function to find the global minimizer from all stationary points, which are given by the function creation\n",
    "def get_global_minimizer(problem, stat_points):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    problem: function which takes x coordinates to calculate y axis values\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    stat_points: array of stationary points, which should be compared\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    for stat_point in np.array(stat_points).astype(float):\n",
    "        y.append([problem(stat_point), stat_point])\n",
    "    y = np.array(y)\n",
    "    return y[y[:,0].argsort()][0, 1]\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# problem_x(x) --> is the problem function f, which is used by algorithm.\n",
    "# derivative_problem_X(x) --> as the name implies, this function is the derivative of the problem function f (problem_X)\n",
    "# second_derivative_problem_X(x) --> as the name implies, this function is the second derivative of the problem function f (problem_X)\n",
    "# in all cases you can parse x coordinates to the functions in order to receive the corresponding y axis values.\n",
    "#############################################################################################################################################################\n",
    "# Problem 6\n",
    "\n",
    "def problem_6(x):\n",
    "    return -(x*(3*x**3+(-4*abc_6[2]-4*abc_6[1]-4*abc_6[0])*x**2+((6*abc_6[1]+6*abc_6[0])*abc_6[2]+6*abc_6[0]*abc_6[1])*x-12*abc_6[0]*abc_6[1]*abc_6[2]))/12\n",
    "\n",
    "def derivative_problem_6(x):\n",
    "    return (abc_6[0]-x)*(abc_6[1]-x)*(abc_6[2]-x)\n",
    "\n",
    "def second_derivative_problem_6(x):\n",
    "    return -(abc_6[1]-x)*(abc_6[2]-x)-(abc_6[0]-x)*(abc_6[2]-x)-(abc_6[0]-x)*(abc_6[1]-x)\n",
    "#############################################################################################################################################################\n",
    "# Problem 7\n",
    "\n",
    "def problem_7(x):\n",
    "    return -(x*(3*x**3+(-4*abc_7[2]-4*abc_7[1]-4*abc_7[0])*x**2+((6*abc_7[1]+6*abc_7[0])*abc_7[2]+6*abc_7[0]*abc_7[1])*x-12*abc_7[0]*abc_7[1]*abc_7[2]))/12\n",
    "\n",
    "def derivative_problem_7(x):\n",
    "    return (abc_7[0]-x)*(abc_7[1]-x)*(abc_7[2]-x)\n",
    "\n",
    "def second_derivative_problem_7(x):\n",
    "    return -(abc_7[1]-x)*(abc_7[2]-x)-(abc_7[0]-x)*(abc_7[2]-x)-(abc_7[0]-x)*(abc_7[1]-x)\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 8\n",
    "\n",
    "def problem_8(x):\n",
    "    return -(x*(3*x**3+(-4*abc_8[2]-4*abc_8[1]-4*abc_8[0])*x**2+((6*abc_8[1]+6*abc_8[0])*abc_8[2]+6*abc_8[0]*abc_8[1])*x-12*abc_8[0]*abc_8[1]*abc_8[2]))/12\n",
    "\n",
    "def derivative_problem_8(x):\n",
    "    return (abc_8[0]-x)*(abc_8[1]-x)*(abc_8[2]-x)\n",
    "\n",
    "def second_derivative_problem_8(x):\n",
    "    return -(abc_8[1]-x)*(abc_8[2]-x)-(abc_8[0]-x)*(abc_8[2]-x)-(abc_8[0]-x)*(abc_8[1]-x)\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 9\n",
    "\n",
    "def problem_9(x):\n",
    "    return -(x*(3*x**3+(-4*abc_9[2]-4*abc_9[1]-4*abc_9[0])*x**2+((6*abc_9[1]+6*abc_9[0])*abc_9[2]+6*abc_9[0]*abc_9[1])*x-12*abc_9[0]*abc_9[1]*abc_9[2]))/12\n",
    "\n",
    "def derivative_problem_9(x):\n",
    "    return (abc_9[0]-x)*(abc_9[1]-x)*(abc_9[2]-x)\n",
    "\n",
    "def second_derivative_problem_9(x):\n",
    "    return -(abc_9[1]-x)*(abc_9[2]-x)-(abc_9[0]-x)*(abc_9[2]-x)-(abc_9[0]-x)*(abc_9[1]-x)\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Problem 10\n",
    "\n",
    "def problem_10(x):\n",
    "    return -(x*(3*x**3+(-4*abc_10[2]-4*abc_10[1]-4*abc_10[0])*x**2+((6*abc_10[1]+6*abc_10[0])*abc_10[2]+6*abc_10[0]*abc_10[1])*x-12*abc_10[0]*abc_10[1]*abc_10[2]))/12\n",
    "\n",
    "def derivative_problem_10(x):\n",
    "    return (abc_10[0]-x)*(abc_10[1]-x)*(abc_10[2]-x)\n",
    "\n",
    "def second_derivative_problem_10(x):\n",
    "    return -(abc_10[1]-x)*(abc_10[2]-x)-(abc_10[0]-x)*(abc_10[2]-x)-(abc_10[0]-x)*(abc_10[1]-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "DEFAULT TESTS WITHOUT ANY ADDITIONAL FEATURES\n",
      "============================================================================\n",
      "**** PROBLEM 6 ****\n",
      "performed 43127 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[9.99872796e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 7 ****\n",
      "performed 9827 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t50.0\n",
      "Approximated x is :\t[[50.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-9062500.0\n",
      "Function value in approximated point:   [[-9062500.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[9.99020884e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 8 ****\n",
      "performed 4213 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t80.0\n",
      "Approximated x is :\t[[80.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-53824000.0\n",
      "Function value in approximated point:   [[-53823999.99999999]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[9.97741182e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 9 ****\n",
      "performed 23244 iterations.\n",
      "Initial x is :\t\t[[90.]]\n",
      "Optimal x is :\t\t100.0\n",
      "Approximated x is :\t[[100.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-24166666.666666668\n",
      "Function value in approximated point:   [[-24166666.66666669]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-9.99893928e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "**** PROBLEM 10 ****\n",
      "performed 24993 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t120.0\n",
      "Approximated x is :\t[[120.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-38016000.0\n",
      "Function value in approximated point:   [[-38015999.99999998]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-9.99783197e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "x_start = np.array([[100]], dtype=float)  # start point for all problems\n",
    "print(\"============================================================================\")\n",
    "print(\"DEFAULT TESTS WITHOUT ANY ADDITIONAL FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 6 ****\")\n",
    "x_hat = steepest_descent_uni(x_start, problem_6,  derivative_problem_6)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 7 ****\")\n",
    "x_hat=steepest_descent_uni(x_start, problem_7,  derivative_problem_7)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_7, abc_7), x_appr=x_hat, f=problem_7, grad=derivative_problem_7, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 8 ****\")\n",
    "x_hat=steepest_descent_uni(x_start, problem_8,  derivative_problem_8)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_8, abc_8), x_appr=x_hat, f=problem_8, grad=derivative_problem_8, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 9 ****\")\n",
    "x_start = np.array([[90]], dtype=float)  # start point for all problems\n",
    "x_hat=steepest_descent_uni(x_start, problem_9,  derivative_problem_9)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_9, abc_9), x_appr=x_hat, f=problem_9, grad=derivative_problem_9, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"**** PROBLEM 10 ****\")\n",
    "x_start = np.array([[100]], dtype=float)  # start point for all problems\n",
    "x_hat=steepest_descent_uni(x_start, problem_10,  derivative_problem_10)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_10, abc_10), x_appr=x_hat, f=problem_10, grad=derivative_problem_10, args={}, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED ADVANCED STOPPING CRITERION FOR PROBLEM 6\n",
      "============================================================================\n",
      "*** Y DIFFERENCE SMALLER THAN ADJUSTED TOLERANCE ***\n",
      "performed 20629 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.00029749]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-327999.99976459]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[1.58265798]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[False]]\n",
      "============================================================================\n",
      "VERSUS NORMAL STOPPING CRITERION FOR PROBLEM 6\n",
      "============================================================================\n",
      "performed 43127 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[9.99872796e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED ADVANCED STOPPING CRITERION FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_start = np.array([[100]], dtype=float)  # start point for all problems\n",
    "x_hat = steepest_descent_uni(x_start, problem_6,  derivative_problem_6, activate_new_stop_crit=True)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL STOPPING CRITERION FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_hat = steepest_descent_uni(x_start, problem_6,  derivative_problem_6, activate_new_stop_crit=False)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "ADDITIONAL TESTS WITH ACTIAVTED FEATURES\n",
      "============================================================================\n",
      "ACTIVATED APPROXIMATED GRADIENT FOR PROBLEM 6\n",
      "============================================================================\n",
      "performed 42912 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[1.03201677e-05]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n",
      "============================================================================\n",
      "VERSUS NORMAL GRADIENT FOR PROBLEM 6\n",
      "============================================================================\n",
      "performed 43127 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t[[20.]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   [[-328000.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[9.99872796e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================\")\n",
    "print(\"ADDITIONAL TESTS WITH ACTIAVTED FEATURES\")\n",
    "print(\"============================================================================\")\n",
    "print(\"ACTIVATED APPROXIMATED GRADIENT FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_start = np.array([[100]], dtype=float)  # start point for all problems\n",
    "x_hat = steepest_descent_uni(x_start, problem_6,  derivative_problem_6, activate_alt_gradient=True)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "print(\"VERSUS NORMAL GRADIENT FOR PROBLEM 6\")\n",
    "print(\"============================================================================\")\n",
    "x_hat = steepest_descent_uni(x_start, problem_6,  derivative_problem_6, activate_alt_gradient=False)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>\n",
    "\n",
    "### ANALYSIS:\n",
    "\n",
    "#### Stopping Criterion:\n",
    "\n",
    "Our Steepest Algorithm is able to find all global minimizers, which is already really good. In some cases the algorithm is not able to achieve an error rate below 0.001, but this happens not often. <br>We could easily improve this by lowering the epsilon (tolerance), but this is also increases the runtime of the algorithm, therefore we decided to take the middle route and accept some \"higher\" errors. The errors are still below the min requirement of 0.01/0.001. <br><br>\n",
    "Additionally we tested our <b>advanced</b> stopping criterion, which yields major imporvements for the runtime. We were able to reduce the number of iterations by about 10k.<br>\n",
    "This is really helpful, if we want to increase the speed of our algorithm. Everything has its price, in this case we loose some accuracy and the error increases.<br> We still have a reasonable low error, but one has to be aware of that.\n",
    "\n",
    "#### Approximated Gradient:\n",
    "\n",
    "As we can see above, the approxmation algorithm works perfectly. There is only a small difference between the results. If the difference would be larger, <br>\n",
    "we would notice that the algorithm converges to infinity and would be able to find the minimizer. Therefore it is really important to find the right tolerance.<br>\n",
    "Again as already mentioned in the stability section, we have to find the middle way, between low error and fast execution.\n",
    "\n",
    "\n",
    "## Comparison:\n",
    "\n",
    "#### Steepest Descent (CURRENT):\n",
    "\n",
    "Needs rescaling for unbalanced problem function.<br>\n",
    "Needs many iterations >20k to find minimizer.<br>\n",
    "Does not reach lowest error bound, with advanced stopping criterion. This could be fixed, but runtime would be higher.<br>\n",
    "Stable as long problems are not too complicated (unbalanced, many local min, etc.)<br>\n",
    "\n",
    "#### Conjugate Gradient:\n",
    "\n",
    "Needs rescaling for unbalanced problem function.<br>\n",
    "Needs many iterations >20k to find minimizer.<br>\n",
    "Does not reach lowest error bound, with advanced stopping criterion. This could be fixed, but runtime would be higher. (Better than steepest descent, still not optimal)<br>\n",
    "Stable as long problems are not too complicated (unbalanced, many local min, etc.)<br>\n",
    "\n",
    "#### Newton:\n",
    "\n",
    "Does not need rescaling for unbalanced problem function. Newton default behaviour.<br>\n",
    "Really fast <10 iterations for our examples to find solution.<br>\n",
    "Reaches lowest error bound with and without additional features.<br>\n",
    "Stable with and without additional features, this could change for more difficult problems, but has not been tested <br>\n",
    "\n",
    "#### Quasi Netwon:\n",
    "\n",
    "\n",
    "Does not need rescaling for unbalanced problem function. Newton default behaviour.<br>\n",
    "Really fast <30 iterations for our examples to find solution.<br>\n",
    "Reaches lowest error bound with and without additional features.<br>\n",
    "Stable with and without additional features, this could change for more difficult problems, but has not been tested <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
    "Template should include sceletons for:<ul>\n",
    "    <li>custom function to optimise over </li> \n",
    "    <li>values initialisation to submit into otimising algorithm </li> \n",
    "    <li>optimiser function call</li> \n",
    "    <li>report print out call</li> </ul><br>\n",
    "Provide descriptions and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Y DIFFERENCE SMALLER THAN ADJUSTED TOLERANCE ***\n",
      "performed 2754 iterations.\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]]\n",
      "Optimal x is :\t\t[[1]\n",
      " [3]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [2]\n",
      " [4]\n",
      " [5]\n",
      " [4]]\n",
      "Approximated x is :\t[[1.00192138]\n",
      " [3.00344123]\n",
      " [8.99699249]\n",
      " [7.99813051]\n",
      " [8.99548132]\n",
      " [7.99661354]\n",
      " [1.99861108]\n",
      " [4.00166592]\n",
      " [5.00208357]\n",
      " [4.00423351]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-3911.5]]\n",
      "Function value in approximated point:   [[-3911.49999833]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[0.0006771 ]\n",
      " [0.00101023]\n",
      " [0.00010752]\n",
      " [0.00073349]\n",
      " [0.00042642]\n",
      " [0.00037289]\n",
      " [0.00083196]\n",
      " [0.0007534 ]\n",
      " [0.00071559]\n",
      " [0.00043687]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "==================================================================================================================================\n",
      "*** Y DIFFERENCE SMALLER THAN ADJUSTED TOLERANCE ***\n",
      "performed 3545 iterations.\n",
      "Initial x is :\t\t[[100.]]\n",
      "Optimal x is :\t\t200.0\n",
      "Approximated x is :\t[[199.99990897]]\n",
      "Is close verificaion: \t[[ True]]\n",
      "\n",
      "Function value in optimal point:\t-386666666.6666667\n",
      "Function value in approximated point:   [[-3.86666667e+08]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-3.60474152]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**** Teacher Test ****\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "** Custom Function to optimise over\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "- All problems, quadratic and non quadratic are based on the instruction from project phase 1.\n",
    "*** Quadratic problems\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "- One can easily create new quadratic problems by changing the seed of the \"get_random_A_b_minimizer\" function. This function uses the seed to create random matrices, which are than used for the problem function.\n",
    "- The three functions problem_x_q, grad_problem_x and hessian_problem_x are used by the optimizer and those functions use the return values of the get_random_A_b_minimizer functions to create the problem function.\n",
    "- Therefore one can change the seed in the mentioned function and create a new problem, which can be recreated and is still random.\n",
    "- It is also possible to create completly different problems, but the structure has to match.\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "*** Non Quadratic problems\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "- One can also create new non quadratic problems by changing the values of the \"abc_x\" array. Those are the pre defined stationary points of the problem function. By choosing different points, the problem changes.\n",
    "- The three functions problem_x, derivative_problem_x and second_derivative_problem_x are used by the optimizer and those functions use the abc_x array to create the problem function.\n",
    "- Therefore one can change the content of the abc_x array and create a new problem. \n",
    "- !!!!!!!!!!!!!!!IMPORTANT!!!!!!!!!!!!!!!!!!!!: No additional values should be entered, this means the length of this array should be 3, otherwise the solution might not match.\n",
    "- It is also possible to create completly different problems, but the structure has to match.\n",
    "\"\"\"\n",
    "\n",
    "def get_random_A_b_minimizer(seed):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    seed: Seed is an int value, which is used to create reproducable random values.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    - 10x10 random matrix is produced by this function, which is the base of the problem\n",
    "    - 10x1 random vector is produced, which is also the minimizer of the function and the goal of the algo\n",
    "    - Function uses thoes variables to create a problem function, which can be solved by the algo\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    A=np.random.randint(0, 2, (10, 10))\n",
    "    A=A@A.T\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    minimizer=np.random.randint(0, 10, (10, 1))\n",
    "\n",
    "    b=A@minimizer\n",
    "\n",
    "    return A, b, minimizer\n",
    "\n",
    "# Quadratic problem\n",
    "A_6, b_6, minimizer_6 = get_random_A_b_minimizer(99)\n",
    "\n",
    "def problem_6_q(x):\n",
    "    return (x.T@A_6@x)/2-b_6.T@x\n",
    "\n",
    "def grad_problem_6_q(x):\n",
    "    return np.dot(A_6,x)-b_6\n",
    "\n",
    "def hessian_problem_6_q(x):\n",
    "    return A_6\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Non quadratic problem\n",
    "\n",
    "abc_11 = np.array([2, 200, 400])\n",
    "def problem_11(x):\n",
    "    return -(x*(3*x**3+(-4*abc_11[2]-4*abc_11[1]-4*abc_11[0])*x**2+((6*abc_11[1]+6*abc_11[0])*abc_11[2]+6*abc_11[0]*abc_11[1])*x-12*abc_11[0]*abc_11[1]*abc_11[2]))/12\n",
    "\n",
    "def derivative_problem_11(x):\n",
    "    return (abc_11[0]-x)*(abc_11[1]-x)*(abc_11[2]-x)\n",
    "\n",
    "def second_derivative_problem_11(x):\n",
    "    return -(abc_11[1]-x)*(abc_11[2]-x)-(abc_11[0]-x)*(abc_11[2]-x)-(abc_11[0]-x)*(abc_11[1]-x)\n",
    "\n",
    "\"\"\"\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "** Value Initalisation Optimizer & Report Call \n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "- Call function to optimise over and safe result\n",
    "- Optimizer function is called with \"steppest_descent_multi\" for quadratic problems and \"steppest_descent_uni\" for non quadratic.\n",
    "- The names come from the number of variables used. There is a strict differnce in our functions between gradient and derivative. It would be possible to use the same function for both problems, but \n",
    "- kept it consistent to the individual task\n",
    "- Decide on a example. \n",
    "-- There are 5 quadratic problems named problem_X_q (q for quadratic), the gradient is than named grad_problem_X_q and same for the hessian (X == Number of problem 1..5)\n",
    "-- There are also 5 non quadratic problems named problem_X, the derivative is than named derivative_problem_X and same for the second derivative (X == Number of problem 6..10)\n",
    "- We have to also define a starting point for the optimizer. All of our problems, in the quadratic case, have to in shape 10x1, since our quadratic problems have 10 variables\n",
    "- The non quadratic problems need a 1x1 starting point, since our problems have only 1 variable.\n",
    "- Additionally one has to choose the extra features. For Steepest descent we implemented the advanced stopping criterion and the approximated gradient and scaling (only for multi). Inverse is not implemented, for reasons checkout the corresponding chapters.\n",
    "- For the final printout, we use the given function. There is a difference between quadratic and non quadratic problems. Our non quadratic problems have 3 stationary points and we have to call the function\n",
    "  get_global_minimizer to find the real minimizer. This has to be done for the non quadratic problems.\n",
    "- x_optimal is for the quadratic problem the variable called \"minimizer_X\", where X has to be replaced by the problem number (1..5)\n",
    "- For the non quadratic problems we have an array called \"abc_X\", where X has to be replaced by the problem number (6..10), which contains all stationary points.\n",
    "\"\"\"\n",
    "##########################################################################################################################################################################################\n",
    "# Quadratic problem call\n",
    "\n",
    "x_start = np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3]]) # 10x1 start point for optimizer\n",
    "problem = problem_6_q # Choose 3rd quadratic problem as problem to be solved by the optimizer\n",
    "gradient_problem = grad_problem_6_q # Choose 3rd quadratic problem gradient to be solved by the optimizer\n",
    "x_optimal = minimizer_6\n",
    "# We could choose different problems and problem gradients, but that does not make sense, therefore it is not recommended :).\n",
    "alt_gradient_flag = True # Here one can activate (True), the approximated gradient or deactivate (False) it.\n",
    "alt_stopping_crit = True # Here one can activate (True), the advanced stopping criterion or deactivate (False) it.\n",
    "activate_pre_scaling = True # Here one can activate (True), the pre scaling or deactivate (False) it.\n",
    "scaling = 1000 # Here on can choose the caling value, which should be used by algorithm\n",
    "\n",
    "# Optimizer Function Call. All the above defined options are parsed to the optimizer call\n",
    "x_hat = steepest_descent_multi(x_start, problem,  gradient_problem, activate_alt_gradient=alt_gradient_flag, activate_new_stop_crit=alt_stopping_crit, activate_pre_scaling=activate_pre_scaling, scaling=scaling)\n",
    "# Final Printout uses the result of the optimizer + the given statements and verifys the results. Args is not used, since our algorithm and functions do not support it.\n",
    "final_printout(x_0=x_start, x_optimal=x_optimal, x_appr=x_hat, f=problem, grad=gradient_problem, args={}, tolerance=1e-2)\n",
    "print(\"==================================================================================================================================\")\n",
    "##########################################################################################################################################################################################\n",
    "# Non Quadratic problem call\n",
    "\n",
    "x_start = np.array([[100]], dtype=float) # 1x1 start point for optimizer\n",
    "problem = problem_11 # Choose 6th non quadratic problem as problem to be solved by the optimizer\n",
    "gradient_problem = derivative_problem_11 # Choose 6th non quadratic problem derivaitve to be solved by the optimizer\n",
    "x_optimal = get_global_minimizer(problem_11, abc_11)\n",
    "# We could choose different problems and problem gradients, but that does not make sense, therefore it is not recommended :).\n",
    "alt_gradient_flag = False # Here one can activate (True), the approximated gradient or deactivate (False) it.\n",
    "alt_stopping_crit = True # Here one can activate (True), the advanced stopping criterion or deactivate (False) it.\n",
    "\n",
    "x_hat = steepest_descent_uni(x_start, problem,  gradient_problem, activate_alt_gradient=alt_gradient_flag, activate_new_stop_crit=alt_stopping_crit)\n",
    "final_printout(x_0=x_start, x_optimal=x_optimal, x_appr=x_hat, f=problem, grad=gradient_problem, args={}, tolerance=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
