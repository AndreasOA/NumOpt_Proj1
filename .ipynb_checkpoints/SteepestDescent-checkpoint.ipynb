{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Group 7<br>\n",
    "Participants information in alphabetical order</i>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">Florian</td>\n",
    "    <td style = \"text-align: left\">Rothkegel</td>\n",
    "    <td style = \"text-align: left\">k11908775</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Andreas</td>\n",
    "    <td style = \"text-align: left\">Oberdammer</td>\n",
    "    <td style = \"text-align: left\">k11908776</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">3</td>\n",
    "    <td style = \"text-align: left\">Martin</td>\n",
    "    <td style = \"text-align: left\">Zwiffl</td>\n",
    "    <td style = \"text-align: left\">k11910668</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">4</td>\n",
    "    <td style = \"text-align: left\">Martin</td>\n",
    "    <td style = \"text-align: left\">Stockinger</td>\n",
    "    <td style = \"text-align: left\">k01035089</td>\n",
    "    </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">5</td>\n",
    "    <td style = \"text-align: left\">Alexander</td>\n",
    "    <td style = \"text-align: left\">Mair</td>\n",
    "    <td style = \"text-align: left\">k11916624</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">6</td>\n",
    "    <td style = \"text-align: left\">Dominik</td>\n",
    "    <td style = \"text-align: left\">Zauner</td>\n",
    "    <td style = \"text-align: left\">k11717988</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
    "<i>Describe how to install additional packages, if you have some, here</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for stopping criterium\n",
    "def stopping_criteria(alpha, iteration, epsilon=1e-5, max_steps=1e6):\n",
    "    # @alpha: alpha in this case can be the derivative/gradient or alpha itself\n",
    "    # @iteration: current iteration of the algorithm\n",
    "    # @epsilon: stopping value for alpha/graident. if alpha is smaller or equal\n",
    "    #           to epsilon the algorithm has to stop\n",
    "    # @max_steps: stopping value for iteration. if iteration is higher or equal\n",
    "    #             to max_steps the algorithm has to stop\n",
    "    # ToDo: Adjustments? Different Name for alpha? Should we parse more?\n",
    "    #       x_old/x_new, y_new/y_old?\n",
    "    return ((alpha <= epsilon) or (iteration >= max_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scaling</h1>\n",
    "    \n",
    "Scaling is a major topic when it comes to improving the performance and stability of an optimization algorithm. Generally, it can be viewed as harmonizing the magnitudes of variables. Scaling does not seem to have a pinpoint definition in literature but at the same time seems to be commonly understood. In our examples, we want to take a look at scaling by variable transformation:\n",
    "    \n",
    "<i>\"Scaling by variable transformation converts the variables from units that typically reflect the physical nature of the problem to units that display certain desirable properties during the minimization process.\"</i> - Gill, Murray, Wright (1981): Practical Optimization\n",
    "\n",
    "Also, in some online resources we find an explanation of : <i>\"Scale x so its components are “around 1”\"</i> <a href=\"http://www.fitzgibbon.ie/optimization-parameter-scaling\">LINK</a>\n",
    "\n",
    "Also, it should be noted that some algorithms are sensible to scaledness/unscaledness of a problem. One of those is Steepest Descent/Gradient Descent. Therefore we will take a look at our Gradient Descent implementation regarding scaling.\n",
    "\n",
    "To line out the basic idea more practically, consider the following simple objective functions that should be minimized:\n",
    "\n",
    "<ul> \n",
    "    <li>$f(x,y) = 1000x^2+10y^2$</li>\n",
    "    <li>$f(x,y) = 10^6x^2+10^6y^2$</li> \n",
    "</ul>\n",
    "\n",
    "The first function can be scaled by a diagonal matrix $D=\\begin{bmatrix}\\frac{1}{1000} & 0 \\\\ 0 & \\frac{1}{10} \\end{bmatrix}$\n",
    "\n",
    "The second function can be viwed as scaled by just dividing it straightforward by $10^6$.\n",
    "\n",
    "But just scaling the the inputs does not really affect the inner workings of our algorithms. We make use of the logic shown in our example scaled Gradient Descent below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scaling and our problems</h3>\n",
    "\n",
    "Generally, since the entries of our base matrices A_x for our multivariate Problems range only between 0 and 9, the influences of single variables are somewhat of similar magnitude. Simplifiying this thought by stating that the term $x^TAx$ comes out the polynomials with factors of magnitude 10 - 100, we argue that we can scale down the variables just by $\\frac{1}{10}$ to get closer to the goal of <i>\"scaling x to, so its components are around 1\"</i>. So we do not use scaling diagonal matrix but simply a factor (can be seen as a diagonal matrix with all the same values).\n",
    "\n",
    "In a nutshell, scaling should not be super-crucial for our somewhat nicely scaled problems. As we will see later, we can still save some iterations on our implementation. If we had many more strongly diverging magnitudes of variable impact, it can become a key point for success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def problem_sclaing(xk, gradient, scaling, activate_new_stop_crit, epsilon, max_steps, objective):\n",
    "        \"\"\"\n",
    "        Scaling:\n",
    "        - Scale xk with the scaling factor in every iteration to get yk\n",
    "        - Evaluate gradient/objective on initial unscaled xk\n",
    "        - Get step size\n",
    "        - Then operate on the scaled yk\n",
    "        - The result needs to be rescaled upon returning\n",
    "        \"\"\"\n",
    "        yk = scaling * xk\n",
    "        i = 0\n",
    "        while True:\n",
    "            xk = yk / scaling\n",
    "            grad = gradient(xk) * scaling\n",
    "            grad_norm = np.linalg.norm(grad / scaling)\n",
    "\n",
    "            if activate_new_stop_crit and stopping_criteria(grad_norm, i, epsilon, max_steps):\n",
    "                break\n",
    "\n",
    "            elif not activate_new_stop_crit and grad_norm <= epsilon:\n",
    "                break\n",
    "\n",
    "            alpha = backtracking_line_search(xk, objective=objective, gradient=gradient, alpha=1, phi=0.9, c=0.0001)\n",
    "            yk = yk - (alpha * grad)\n",
    "            i += 1\n",
    "\n",
    "        print(\"performed \" + str(i+1) + \" iterations. Final norm of gradient: \" + str(grad_norm))\n",
    "\n",
    "        return yk / scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for stabilising goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
    "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for invertion goes here\n",
    "def lu_deco_inverse(A):\n",
    "    n = A.shape[0]\n",
    "    a = np.copy(A.astype(float))\n",
    "    p = np.eye(n)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        j = i + np.argmax(np.abs(a[i:, i]))     # maximum in column\n",
    "        if np.abs(a[i, j]) < 1e-8:\n",
    "            raise ValueError(\"Matrix is singular\")\n",
    "\n",
    "        if i != j:\n",
    "            a[[i, j]] = a[[j, i]]\n",
    "            p[[i, j]] = p[[j, i]]\n",
    "\n",
    "        for i_row in range(i + 1, n):\n",
    "            a[i_row, i] = a[i_row:, i] / a[i, i]\n",
    "        for i_row in range(i + 1, n):\n",
    "            for i_col in range(i + 1, n):\n",
    "                a[i_row, i_col] = a[i_row, i_col] - np.outer(a[i_row, i], a[i, i_col])\n",
    "\n",
    "    a_inv = np.copy(np.dot(p, np.eye(n)))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            a_inv[i] -= np.dot(a[i, j], a_inv[j])         # Forward substitution\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        for j in range(i + 1, n):\n",
    "            a_inv[i] = (a_inv[i] - np.dot(a[i, j], a_inv[j])) / a[i, i]       # Backward substitution\n",
    "\n",
    "    return a_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your function for gradient approximation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "\n",
    "# We added a line-search to our groups algorithm to try it with a fixed learning rate and an iterative line-search\n",
    "def backtracking_line_search(x, objective, gradient, alpha, phi, c):\n",
    "    pk = - gradient(x)\n",
    "    left = objective(x + alpha * pk)\n",
    "    right = objective(x) + c * alpha * (gradient(x).reshape(-1) @ pk.reshape(-1))\n",
    "    while left > right:\n",
    "        alpha = alpha * phi\n",
    "        left = objective(x + alpha * pk)\n",
    "        right = objective(x) + c * alpha * (gradient(x).reshape(-1) @ pk.reshape(-1))\n",
    "    return alpha\n",
    "\n",
    "# Our Main SD algorithm for multivariate functions\n",
    "def steepest_descent_multi(xk, minimizer, objective, gradient, max_steps=1e6, epsilon=1e-5,\n",
    "         activate_new_stop_crit=False,\n",
    "         activate_pre_scaling=False,\n",
    "         activate_alt_gradient=False,\n",
    "         activate_new_matrix_inversion=False,\n",
    "         scaling=None):\n",
    "    \n",
    "    if not activate_pre_scaling:\n",
    "        i = 0\n",
    "        while True:\n",
    "            grad = gradient(xk)\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "            if activate_new_stop_crit and stopping_criteria(grad_norm, i, epsilon, max_steps):\n",
    "                break\n",
    "\n",
    "            elif not activate_new_stop_crit and grad_norm <= epsilon:\n",
    "                break\n",
    "\n",
    "            alpha = backtracking_line_search(xk, objective=objective, gradient=gradient, alpha=1, phi=0.9, c=0.0001)\n",
    "            xk = xk - (alpha * grad)\n",
    "            i += 1\n",
    "\n",
    "        print(\"performed \" + str(i+1) + \" iterations. Final norm of gradient: \" + str(grad_norm))\n",
    "\n",
    "        return xk\n",
    "\n",
    "    elif activate_pre_scaling:\n",
    "        return  problem_sclaing(xk, gradient, scaling, activate_new_stop_crit, epsilon, max_steps, objective)\n",
    "    \n",
    "\n",
    "\n",
    "# Our Main SD algorithm for univariate functions\n",
    "def steepest_descent_uni(xk, minimizers, derivative, lr=1e-7, max_steps=1e6, epsilon=1e-5,\n",
    "         activate_new_stop_crit=False,\n",
    "         activate_pre_scaling=False,\n",
    "         activate_alt_gradient=False,\n",
    "         activate_new_matrix_inversion=False):\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        der = derivative(xk)\n",
    "\n",
    "        if activate_new_stop_crit and stopping_criteria(abs(der), i, epsilon, max_steps):\n",
    "            break\n",
    "\n",
    "        elif not activate_new_stop_crit and abs(der) <= epsilon:\n",
    "            break\n",
    "\n",
    "        xk=xk-(lr * der)\n",
    "        i += 1\n",
    "    print(\"performed \" + str(i+1) + \" iterations. Final derivative: \" + str(der))\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def get_random_A_b_minimizer(seed):\n",
    "    np.random.seed(seed)\n",
    "    A=np.random.randint(0, 2, (11, 11))\n",
    "    A=A@A.T\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    minimizer=np.random.randint(0, 10, (11, 1))\n",
    "\n",
    "    b=A@minimizer\n",
    "\n",
    "    return A, b, minimizer\n",
    "\n",
    "A_1, b_1, minimizer_1 = get_random_A_b_minimizer(0)\n",
    "A_2, b_2, minimizer_2 = get_random_A_b_minimizer(2)\n",
    "A_3, b_3, minimizer_3 = get_random_A_b_minimizer(3)\n",
    "A_4, b_4, minimizer_4 = get_random_A_b_minimizer(4)\n",
    "A_5, b_5, minimizer_5 = get_random_A_b_minimizer(6)\n",
    "A_6, b_6, minimizer_6 = get_random_A_b_minimizer(12)\n",
    "A_7, b_7, minimizer_7 = get_random_A_b_minimizer(15)\n",
    "\n",
    "def problem_1_q(x):\n",
    "    return (x.T@A_1@x)/2-b_1.T@x\n",
    "\n",
    "def grad_problem_1_q(x):\n",
    "    return np.dot(A_1,x)-b_1\n",
    "\n",
    "def hessian_problem_1_q(x):\n",
    "    return A_1\n",
    "\n",
    "def problem_2_q(x):\n",
    "    return (x.T@A_2@x)/2-b_2.T@x\n",
    "\n",
    "def grad_problem_2_q(x):\n",
    "    return A_2@x-b_2\n",
    "\n",
    "def hessian_problem_2_q(x):\n",
    "    return A_2\n",
    "\n",
    "def problem_3_q(x):\n",
    "    return (x.T@A_3@x)/2-b_3.T@x\n",
    "\n",
    "def grad_problem_3_q(x):\n",
    "    return A_3@x-b_3\n",
    "\n",
    "def hessian_problem_3_q(x):\n",
    "    return A_3\n",
    "\n",
    "def problem_4_q(x):\n",
    "    return (x.T@A_4@x)/2-b_4.T@x\n",
    "\n",
    "def grad_problem_4_q(x):\n",
    "    return A_4@x-b_4\n",
    "\n",
    "def hessian_problem_4_q(x):\n",
    "    return A_4\n",
    "\n",
    "def problem_5_q(x):\n",
    "    return (x.T@A_5@x)/2-b_5.T@x\n",
    "\n",
    "def grad_problem_5_q(x):\n",
    "    return A_5@x-b_5\n",
    "\n",
    "def hessian_problem_5_q(x):\n",
    "    return A_5\n",
    "\n",
    "def problem_6_q(x):\n",
    "    return (x.T@A_6@x)/2-b_6.T@x\n",
    "\n",
    "def grad_problem_6_q(x):\n",
    "    return A_6@x-b_6\n",
    "\n",
    "def hessian_problem_6_q(x):\n",
    "    return A_6\n",
    "\n",
    "def problem_7_q(x):\n",
    "    return (x.T@A_7@x)/2-b_7.T@x\n",
    "\n",
    "def grad_problem_7_q(x):\n",
    "    return A_7@x-b_7\n",
    "\n",
    "def hessian_problem_7_q(x):\n",
    "    return A_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><b>Note:</b> After every test print out the resulsts. \n",
    "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
    "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_0: numpy 1D array, corresponds to initial point\n",
    "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
    "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
    "                                      **args\n",
    "       Function f returns a scalar output.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
    "                                         function f,\n",
    "                                         args (which are submitted, because you might need\n",
    "                                              to call f(x,**args) inside your gradient function implementation). \n",
    "          Function grad approximates gradient at given point and returns a 1d np array.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    args: dictionary, additional (except of x) arguments to function f\n",
    "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Initial x is :\\t\\t{x_0}')\n",
    "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
    "    print(f'Approximated x is :\\t{x_appr}')\n",
    "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
    "    f_opt = f(x_optimal)\n",
    "    f_appr = f(x_appr)\n",
    "    print(f'Function value in optimal point:\\t{f_opt}')\n",
    "    print(f'Function value in approximated point:   {f_appr}')\n",
    "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
    "    print(f'Gradient approximation in optimal point is:\\n{grad(x_optimal)}\\n')\n",
    "    grad_appr = grad(x_appr)\n",
    "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
    "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performed 5706 iterations. Final norm of gradient: 9.966706748038616e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]\n",
      " [7]]\n",
      "Approximated x is :\t[[ 5.00003714e+00]\n",
      " [-6.48050560e-05]\n",
      " [ 2.99999108e+00]\n",
      " [ 3.00000900e+00]\n",
      " [ 7.00003029e+00]\n",
      " [ 8.99991710e+00]\n",
      " [ 3.00002229e+00]\n",
      " [ 5.00000835e+00]\n",
      " [ 1.99998317e+00]\n",
      " [ 4.00000495e+00]\n",
      " [ 6.99998616e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4286.5]]\n",
      "Function value in approximated point:   [[-4286.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 5.29118847e-06]\n",
      " [-3.73239843e-07]\n",
      " [ 2.74629519e-06]\n",
      " [ 3.24548660e-06]\n",
      " [ 4.72873120e-06]\n",
      " [-2.27297789e-07]\n",
      " [ 2.74195193e-06]\n",
      " [ 3.60402839e-06]\n",
      " [ 1.86380748e-06]\n",
      " [ 2.17074307e-06]\n",
      " [ 1.42061660e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 3216 iterations. Final norm of gradient: 4.98391934965179e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [8]\n",
      " [6]\n",
      " [2]\n",
      " [8]\n",
      " [7]\n",
      " [2]\n",
      " [1]\n",
      " [5]\n",
      " [4]\n",
      " [4]]\n",
      "Approximated x is :\t[[8.00000898]\n",
      " [7.99998382]\n",
      " [6.00002134]\n",
      " [2.00003131]\n",
      " [8.00000011]\n",
      " [6.99997552]\n",
      " [2.00000105]\n",
      " [0.9999769 ]\n",
      " [4.9999809 ]\n",
      " [4.0000003 ]\n",
      " [4.00001   ]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4260.]]\n",
      "Function value in approximated point:   [[-4260.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 1.54475973e-06]\n",
      " [ 7.99087388e-07]\n",
      " [ 2.43179159e-06]\n",
      " [ 2.64279339e-06]\n",
      " [ 9.34032357e-07]\n",
      " [ 4.25690416e-07]\n",
      " [ 1.59750738e-06]\n",
      " [-5.10815283e-07]\n",
      " [-6.38680937e-07]\n",
      " [ 8.09430276e-07]\n",
      " [ 1.99674236e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 5523 iterations. Final norm of gradient: 8.503497604955918e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [9]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [5]\n",
      " [3]\n",
      " [9]\n",
      " [9]\n",
      " [5]]\n",
      "Approximated x is :\t[[7.99996886e+00]\n",
      " [8.99994254e+00]\n",
      " [2.99999659e+00]\n",
      " [7.99996654e+00]\n",
      " [8.00000613e+00]\n",
      " [3.30128409e-05]\n",
      " [5.00003539e+00]\n",
      " [3.00002425e+00]\n",
      " [8.99999149e+00]\n",
      " [9.00003833e+00]\n",
      " [4.99993226e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-5551.5]]\n",
      "Function value in approximated point:   [[-5551.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-2.63736706e-06]\n",
      " [-4.36025556e-06]\n",
      " [-1.20993371e-06]\n",
      " [-3.89088649e-06]\n",
      " [-2.27631563e-06]\n",
      " [-1.61600303e-06]\n",
      " [-1.53970529e-06]\n",
      " [-2.92089251e-06]\n",
      " [-2.03594442e-06]\n",
      " [-1.01480947e-06]\n",
      " [-2.42254178e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 3116 iterations. Final norm of gradient: 9.425278607126519e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[7]\n",
      " [5]\n",
      " [1]\n",
      " [8]\n",
      " [7]\n",
      " [8]\n",
      " [2]\n",
      " [9]\n",
      " [7]\n",
      " [7]\n",
      " [7]]\n",
      "Approximated x is :\t[[7.00002598]\n",
      " [4.99999318]\n",
      " [1.00000376]\n",
      " [8.00001382]\n",
      " [6.99997339]\n",
      " [8.00001059]\n",
      " [2.00001467]\n",
      " [8.99999001]\n",
      " [6.99998018]\n",
      " [6.99999206]\n",
      " [7.00001556]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-6486.5]]\n",
      "Function value in approximated point:   [[-6486.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[4.15065844e-06]\n",
      " [2.04938976e-06]\n",
      " [2.40466704e-06]\n",
      " [2.17668867e-06]\n",
      " [1.18731720e-06]\n",
      " [4.26268650e-06]\n",
      " [3.81873920e-06]\n",
      " [1.64080728e-06]\n",
      " [2.39239128e-06]\n",
      " [2.75063530e-06]\n",
      " [2.59668042e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 36612 iterations. Final norm of gradient: 6.61345236104397e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[9]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [9]\n",
      " [1]\n",
      " [9]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [8]]\n",
      "Approximated x is :\t[[9.00065322e+00]\n",
      " [2.99937005e+00]\n",
      " [3.99918012e+00]\n",
      " [6.56664992e-04]\n",
      " [9.00053169e+00]\n",
      " [9.99447948e-01]\n",
      " [9.00020054e+00]\n",
      " [1.00035882e+00]\n",
      " [3.99941413e+00]\n",
      " [9.99860791e-01]\n",
      " [8.00037551e+00]]\n",
      "Is close verificaion: \t[[False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]]\n",
      "\n",
      "Function value in optimal point:\t[[-4947.]]\n",
      "Function value in approximated point:   [[-4947.]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 2.90769302e-06]\n",
      " [-3.45507459e-07]\n",
      " [-1.31251497e-06]\n",
      " [ 3.13199362e-06]\n",
      " [ 2.98718939e-06]\n",
      " [-8.53411592e-07]\n",
      " [ 2.15467401e-06]\n",
      " [ 1.88169149e-06]\n",
      " [ 1.31776289e-07]\n",
      " [ 8.24696826e-07]\n",
      " [ 2.25812619e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 24221 iterations. Final norm of gradient: 9.905851251225483e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [5]\n",
      " [9]]\n",
      "Approximated x is :\t[[5.99988678e+00]\n",
      " [1.00032721e+00]\n",
      " [1.99999604e+00]\n",
      " [2.99983052e+00]\n",
      " [3.00012397e+00]\n",
      " [2.05353761e-04]\n",
      " [5.99993491e+00]\n",
      " [9.99785522e-01]\n",
      " [4.00006741e+00]\n",
      " [4.99966512e+00]\n",
      " [9.00027132e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]]\n",
      "\n",
      "Function value in optimal point:\t[[-2939.5]]\n",
      "Function value in approximated point:   [[-2939.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[-3.52327515e-06]\n",
      " [-6.31911306e-07]\n",
      " [-2.10380770e-06]\n",
      " [-2.86326178e-06]\n",
      " [-2.26203645e-06]\n",
      " [-1.35755138e-06]\n",
      " [-4.42560378e-06]\n",
      " [-2.15223611e-06]\n",
      " [-2.61913937e-06]\n",
      " [-5.88309322e-06]\n",
      " [-2.00783035e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 5384 iterations. Final norm of gradient: 9.969363999386206e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[8]\n",
      " [5]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [5]\n",
      " [6]\n",
      " [1]\n",
      " [7]\n",
      " [0]]\n",
      "Approximated x is :\t[[ 8.00003211e+00]\n",
      " [ 5.00004789e+00]\n",
      " [ 4.99995998e+00]\n",
      " [ 6.99995610e+00]\n",
      " [-4.15192269e-05]\n",
      " [ 7.00004392e+00]\n",
      " [ 5.00000377e+00]\n",
      " [ 5.99997135e+00]\n",
      " [ 1.00000830e+00]\n",
      " [ 6.99994864e+00]\n",
      " [ 3.05104789e-05]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4039.5]]\n",
      "Function value in approximated point:   [[-4039.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[4.02329738e-06]\n",
      " [5.15508953e-06]\n",
      " [2.39734811e-06]\n",
      " [1.89199898e-06]\n",
      " [1.10666574e-06]\n",
      " [3.78843379e-06]\n",
      " [3.82928761e-06]\n",
      " [7.21936871e-07]\n",
      " [3.27793498e-06]\n",
      " [4.89533477e-07]\n",
      " [2.35655126e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3],[6]])\n",
    "x_hat=steepest_descent_multi(init, minimizer_1, objective=problem_1_q, gradient=grad_problem_1_q, epsilon=1e-5)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_2, objective=problem_2_q, gradient=grad_problem_2_q, epsilon=1e-5)\n",
    "final_printout(init, minimizer_2, x_hat, problem_2_q, grad_problem_2_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_3, objective=problem_3_q, gradient=grad_problem_3_q, epsilon=1e-5)\n",
    "final_printout(init, minimizer_3, x_hat, problem_3_q, grad_problem_3_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_4, objective=problem_4_q, gradient=grad_problem_4_q, epsilon=1e-5)\n",
    "final_printout(init, minimizer_4, x_hat, problem_4_q, grad_problem_4_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_5, objective=problem_5_q, gradient=grad_problem_5_q, epsilon=1e-5)\n",
    "final_printout(init, minimizer_5, x_hat, problem_5_q, grad_problem_5_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_6, objective=problem_6_q, gradient=grad_problem_6_q, epsilon=1e-5)\n",
    "final_printout(init, minimizer_6, x_hat, problem_6_q, grad_problem_6_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_hat=steepest_descent_multi(init, minimizer_7, objective=problem_7_q, gradient=grad_problem_7_q, epsilon=1e-5)\n",
    "final_printout(init, minimizer_7, x_hat, problem_7_q, grad_problem_7_q, {}, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performed 5774 iterations. Final norm of gradient: 8.882322140521558e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]\n",
      " [7]]\n",
      "Approximated x is :\t[[ 5.00003355e+00]\n",
      " [-5.85492389e-05]\n",
      " [ 2.99999194e+00]\n",
      " [ 3.00000813e+00]\n",
      " [ 7.00002737e+00]\n",
      " [ 8.99992510e+00]\n",
      " [ 3.00002014e+00]\n",
      " [ 5.00000754e+00]\n",
      " [ 1.99998479e+00]\n",
      " [ 4.00000447e+00]\n",
      " [ 6.99998750e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4286.5]]\n",
      "Function value in approximated point:   [[-4286.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 4.72126209e-06]\n",
      " [-3.59312267e-07]\n",
      " [ 2.43856917e-06]\n",
      " [ 2.89008491e-06]\n",
      " [ 4.21816239e-06]\n",
      " [-2.37170497e-07]\n",
      " [ 2.44789550e-06]\n",
      " [ 3.20866727e-06]\n",
      " [ 1.65041175e-06]\n",
      " [ 1.93258131e-06]\n",
      " [ 1.25753886e-06]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n",
      "performed 5880 iterations. Final norm of gradient: 7.109876283770015e-06\n",
      "Initial x is :\t\t[[6]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [6]]\n",
      "Optimal x is :\t\t[[5]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]\n",
      " [7]]\n",
      "Approximated x is :\t[[ 5.00002863e+00]\n",
      " [-4.99660009e-05]\n",
      " [ 2.99999311e+00]\n",
      " [ 3.00000693e+00]\n",
      " [ 7.00002335e+00]\n",
      " [ 8.99993608e+00]\n",
      " [ 3.00001718e+00]\n",
      " [ 5.00000643e+00]\n",
      " [ 1.99998702e+00]\n",
      " [ 4.00000381e+00]\n",
      " [ 6.99998933e+00]]\n",
      "Is close verificaion: \t[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "Function value in optimal point:\t[[-4286.5]]\n",
      "Function value in approximated point:   [[-4286.5]]\n",
      "Is close verificaion:\t[[ True]]\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "[[ 3.80017497e-06]\n",
      " [-3.92186706e-07]\n",
      " [ 1.91612031e-06]\n",
      " [ 2.30342903e-06]\n",
      " [ 3.39042197e-06]\n",
      " [-3.25542146e-07]\n",
      " [ 1.97536585e-06]\n",
      " [ 2.55460608e-06]\n",
      " [ 1.27888762e-06]\n",
      " [ 1.53851551e-06]\n",
      " [ 9.72776164e-07]]\n",
      "\n",
      "Is close verificaion:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "# One Problem exemplarily scaled with variable scaling\n",
    "\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3],[6]])\n",
    "x_hat=steepest_descent_multi(init, minimizer_1, objective=problem_1_q, gradient=grad_problem_1_q, epsilon=1e-5, scaling = 0.1)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")\n",
    "init=np.array([[6],[1],[2],[6],[7],[8],[1],[5],[1],[3],[6]])\n",
    "x_hat=steepest_descent_multi(init, minimizer_1, objective=problem_1_q, gradient=grad_problem_1_q, epsilon=1e-5, scaling = 1000)\n",
    "final_printout(init, minimizer_1, x_hat, problem_1_q, grad_problem_1_q, {}, 1e-4)\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In our initial version we used a stable learning rate á la NN gradient descent, but changed it to a simple backtracking line search, which cut the needed iterations massively (from > 100.000 to ~3.000-5.000)</li>\n",
    "    <li>It is hard to tell whether scaling improved stability or something like that, but it apparently cost us a few iterations on our example if we scale down (what we should do according to literature). When we scale up by 1000 as shown, we need a less iterations. Yet, this is not advised theoretically. This could have something to do with the chosen parameters in backtracking line search.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "abc_6 = np.array([1, 20, 300])\n",
    "abc_7 = np.array([4, 50, 600])\n",
    "abc_8 = np.array([7, 80, 900])\n",
    "abc_9 = np.array([1, 100, 200])\n",
    "abc_10 = np.array([2, 120, 200])\n",
    "\n",
    "def get_global_minimizer(problem, stat_points):\n",
    "    y = []\n",
    "    for stat_point in np.array(stat_points).astype(float):\n",
    "        y.append([problem(stat_point), stat_point])\n",
    "    y = np.array(y)\n",
    "    return y[y[:,0].argsort()][0, 1]\n",
    "\n",
    "def problem_6(x):\n",
    "    return -(x*(3*x**3+(-4*abc_6[2]-4*abc_6[1]-4*abc_6[0])*x**2+((6*abc_6[1]+6*abc_6[0])*abc_6[2]+6*abc_6[0]*abc_6[1])*x-12*abc_6[0]*abc_6[1]*abc_6[2]))/12\n",
    "\n",
    "def derivative_problem_6(x):\n",
    "    return (abc_6[0]-x)*(abc_6[1]-x)*(abc_6[2]-x)\n",
    "\n",
    "def second_derivative_problem_6(x):\n",
    "    return -(abc_6[1]-x)*(abc_6[2]-x)-(abc_6[0]-x)*(abc_6[2]-x)-(abc_6[0]-x)*(abc_6[1]-x)\n",
    "\n",
    "def problem_7(x):\n",
    "    return -(x*(3*x**3+(-4*abc_7[2]-4*abc_7[1]-4*abc_7[0])*x**2+((6*abc_7[1]+6*abc_7[0])*abc_7[2]+6*abc_7[0]*abc_7[1])*x-12*abc_7[0]*abc_7[1]*abc_7[2]))/12\n",
    "\n",
    "def derivative_problem_7(x):\n",
    "    return (abc_7[0]-x)*(abc_7[1]-x)*(abc_7[2]-x)\n",
    "\n",
    "def second_derivative_problem_7(x):\n",
    "    return -(abc_7[1]-x)*(abc_7[2]-x)-(abc_7[0]-x)*(abc_7[2]-x)-(abc_7[0]-x)*(abc_7[1]-x)\n",
    "\n",
    "def problem_8(x):\n",
    "    return -(x*(3*x**3+(-4*abc_8[2]-4*abc_8[1]-4*abc_8[0])*x**2+((6*abc_8[1]+6*abc_8[0])*abc_8[2]+6*abc_8[0]*abc_8[1])*x-12*abc_8[0]*abc_8[1]*abc_8[2]))/12\n",
    "\n",
    "def derivative_problem_8(x):\n",
    "    return (abc_8[0]-x)*(abc_8[1]-x)*(abc_8[2]-x)\n",
    "\n",
    "def second_derivative_problem_8(x):\n",
    "    return -(abc_8[1]-x)*(abc_8[2]-x)-(abc_8[0]-x)*(abc_8[2]-x)-(abc_8[0]-x)*(abc_8[1]-x)\n",
    "\n",
    "def problem_9(x):\n",
    "    return -(x*(3*x**3+(-4*abc_9[2]-4*abc_9[1]-4*abc_9[0])*x**2+((6*abc_9[1]+6*abc_9[0])*abc_9[2]+6*abc_9[0]*abc_9[1])*x-12*abc_9[0]*abc_9[1]*abc_9[2]))/12\n",
    "\n",
    "def derivative_problem_9(x):\n",
    "    return (abc_9[0]-x)*(abc_9[1]-x)*(abc_9[2]-x)\n",
    "\n",
    "def second_derivative_problem_9(x):\n",
    "    return -(abc_9[1]-x)*(abc_9[2]-x)-(abc_9[0]-x)*(abc_9[2]-x)-(abc_9[0]-x)*(abc_9[1]-x)\n",
    "\n",
    "def problem_10(x):\n",
    "    return -(x*(3*x**3+(-4*abc_10[2]-4*abc_10[1]-4*abc_10[0])*x**2+((6*abc_10[1]+6*abc_10[0])*abc_10[2]+6*abc_10[0]*abc_10[1])*x-12*abc_10[0]*abc_10[1]*abc_10[2]))/12\n",
    "\n",
    "def derivative_problem_10(x):\n",
    "    return (abc_10[0]-x)*(abc_10[1]-x)*(abc_10[2]-x)\n",
    "\n",
    "def second_derivative_problem_10(x):\n",
    "    return -(abc_10[1]-x)*(abc_10[2]-x)-(abc_10[0]-x)*(abc_10[2]-x)-(abc_10[0]-x)*(abc_10[1]-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performed 420 iterations. Final derivative: 9.629205522687836e-06\n",
      "Initial x is :\t\t100\n",
      "Optimal x is :\t\t20.0\n",
      "Approximated x is :\t20.00000000181\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-328000.0\n",
      "Function value in approximated point:   -327999.99999999994\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "9.629205522687836e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n",
      "============================================================================\n",
      "performed 86 iterations. Final derivative: 8.642493298852665e-06\n",
      "Initial x is :\t\t100\n",
      "Optimal x is :\t\t50.0\n",
      "Approximated x is :\t50.0000000003416\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-9062500.0\n",
      "Function value in approximated point:   -9062499.999999998\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "8.642493298852665e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n",
      "============================================================================\n",
      "performed 29 iterations. Final derivative: 5.185634108789749e-06\n",
      "Initial x is :\t\t100\n",
      "Optimal x is :\t\t80.0\n",
      "Approximated x is :\t80.00000000008663\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-53824000.0\n",
      "Function value in approximated point:   -53824000.00000002\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "5.185634108789749e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n",
      "============================================================================\n",
      "performed 222 iterations. Final derivative: -9.837711445469644e-06\n",
      "Initial x is :\t\t90\n",
      "Optimal x is :\t\t100.0\n",
      "Approximated x is :\t99.99999999900629\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-24166666.666666668\n",
      "Function value in approximated point:   -24166666.66666666\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "-9.837711445469644e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n",
      "============================================================================\n",
      "performed 239 iterations. Final derivative: -9.989917089157452e-06\n",
      "Initial x is :\t\t100\n",
      "Optimal x is :\t\t120.0\n",
      "Approximated x is :\t119.99999999894175\n",
      "Is close verificaion: \tTrue\n",
      "\n",
      "Function value in optimal point:\t-38016000.0\n",
      "Function value in approximated point:   -38015999.999999985\n",
      "Is close verificaion:\tTrue\n",
      "\n",
      "Gradient approximation in optimal point is:\n",
      "-0.0\n",
      "\n",
      "Gradient approximation in approximated point is:\n",
      "-9.989917089157452e-06\n",
      "\n",
      "Is close verificaion:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "x_start=100\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_6), derivative_problem_6, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_6, abc_6), x_appr=x_hat, f=problem_6, grad=derivative_problem_6, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_start=100\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_7), derivative_problem_7, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_7, abc_7), x_appr=x_hat, f=problem_7, grad=derivative_problem_7, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_start=100\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_8), derivative_problem_8, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_8, abc_8), x_appr=x_hat, f=problem_8, grad=derivative_problem_8, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_start=90\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_9), derivative_problem_9, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_9, abc_9), x_appr=x_hat, f=problem_9, grad=derivative_problem_9, args={}, tolerance=1e-4)\n",
    "print(\"============================================================================\")\n",
    "x_start=100\n",
    "x_hat=steepest_descent_uni(x_start, np.array(abc_10), derivative_problem_10, lr=1e-5)\n",
    "final_printout(x_0=x_start, x_optimal=get_global_minimizer(problem_10, abc_10), x_appr=x_hat, f=problem_10, grad=derivative_problem_10, args={}, tolerance=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
    "Template should include sceletons for:<ul>\n",
    "    <li>custom function to optimise over </li> \n",
    "    <li>values initialisation to submit into otimising algorithm </li> \n",
    "    <li>optimiser function call</li> \n",
    "    <li>report print out call</li> </ul><br>\n",
    "Provide descriptions and comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
